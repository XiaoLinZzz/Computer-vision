{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMOJD0_jdzzg"
      },
      "source": [
        "## Computer vision 2022 Assignment 3: Deep Learning for Perception Tasks\n",
        "\n",
        "This assignment contains 2 questions. The first question gives you a basic understanding of the classifier. The second question requires you to write a simple proposal.\n",
        "\n",
        "# Question 1: A simple classifier (60%)\n",
        "\n",
        "For this exercise, we will provide a demo code showing how to train a network on a small dataset called FashionMinst. Please go through the following tutorials first. You will get a basic understanding about how to train an image classification network in pytorch. You can change the training scheme and the network structure. Please answer the following questions then. You can orginaze your own text and code cell to show the answer of each questions.\n",
        "\n",
        "\n",
        "Note: Please plot the loss curve for each experiment (2 point).\n",
        "\n",
        "\n",
        "Requirement:\n",
        "\n",
        "Q1.1 (1 point) Change the learning rate and train for 10 epochs. Fill this table:\n",
        "\n",
        "|Lr|Accuracy|\n",
        "|---|---|\n",
        "|1   |   19.92%   |\n",
        "|0.1|     87.22%     |\n",
        "|0.01|     83.67%    |\n",
        "|0.001  |    87.5%    |\n",
        "\n",
        "\n",
        "Q1.2 (2 point) Report the number of epochs when the accuracy reaches 90%. Fill this table:\n",
        "\n",
        "|Lr|Accuracy|Epoch|\n",
        "|---|---|---|\n",
        "|1   |   10%   |   11  |\n",
        "|0.1|     90%     |  174  |\n",
        "|0.01|    89.04%     |  273  |\n",
        "|0.001  |    87.2%    |   297  |\n",
        "\n",
        "\n",
        "Q1.3 (2 points) Compare the results in table 1 and table 2, what is your observation and your understanding of learning rate?\n",
        "\n",
        "From the table 1 and table 2, I notice that smaller learning rates necessitate more training epochs because of the fewer changes. On the other hand, larger learning rates result in faster changes.\n",
        "\n",
        "Q1.4 (3 point) Build a deeper/ wider network. Report the accuracy and the parameters for each structure. Parameters represent the number of trainable parameters in your model, e.g. a 3 x 3 conv has 9 parameters.\n",
        "\n",
        "|Structures|Accuracy|Parameters|\n",
        "|---|---|---|\n",
        "|Base   |   87.22%   |  669,706|\n",
        "|Deeper|  89.4%        |   674,836|\n",
        "|Wider|    90.3%     |   1,863,690|\n",
        "\n",
        "\n",
        "Q1.5 (2 points) Choose to do one of the following two tasks:\n",
        "\n",
        "a. Write a code to calculate the parameter and expian the code.\n",
        "\n",
        "OR\n",
        "\n",
        "b. Write done the process of how to calculate the parameters by hand. \n",
        "\n",
        "\n",
        "Q1.6 (1 points) What are your observations and conclusions for changing network structure?\n",
        "\n",
        "With the increasing of the parameters, the accuracy will also increase.\n",
        "\n",
        "Q1.7 (2 points) Calculate the mean of the gradients of the loss to all trainable parameters. Plot the gradients curve for the first 100 training steps. What are your observations? Note that this gradients will be saved with the training weight automatically after you call loss.backwards(). Hint: the mean of the gradients should be decreased.\n",
        "\n",
        "For more exlanation of q1.7, you could refer to the following simple instructions: https://colab.research.google.com/drive/1XAsyNegGSvMf3_B6MrsXht7-fHqtJ7OW?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sqkIpLjpVsh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # This is for mathematical operations\n",
        "\n",
        "# this is used in plotting \n",
        "import matplotlib.pyplot as plt \n",
        "import time\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "\n",
        "from a3 import *\n",
        "from torchinfo import summary\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%reload_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import wandb\n",
        "\n",
        "# wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# wandb.init(project=\"Assignment 3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wy3xhEx_x-1"
      },
      "outputs": [],
      "source": [
        "#### Tutorial Code\n",
        "####PyTorch has two primitives to work with data: torch.utils.data.DataLoader and torch.utils.data.Dataset. \n",
        "#####Dataset stores samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset.\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download training data from open datasets. \n",
        "##Every TorchVision Dataset includes two arguments: \n",
        "##transform and target_transform to modify the samples and labels respectively.\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNI4IusI_1ol"
      },
      "source": [
        "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset and supports automatic batching, sampling, shuffling, and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQZ5l5Zs_4C3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
            "Shape of y:  torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "# wandb.log({'batch_size': batch_size})\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMtCU2LO_9Dk"
      },
      "source": [
        "To define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the init function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRSp7pd3_6bS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "NeuralNetwork                            --                        --\n",
              "├─Flatten: 1-1                           [1, 784]                  --\n",
              "├─Sequential: 1-2                        [1, 10]                   --\n",
              "│    └─Linear: 2-1                       [1, 512]                  401,920\n",
              "│    └─ReLU: 2-2                         [1, 512]                  --\n",
              "│    └─Linear: 2-3                       [1, 512]                  262,656\n",
              "│    └─ReLU: 2-4                         [1, 512]                  --\n",
              "│    └─Linear: 2-5                       [1, 10]                   5,130\n",
              "==========================================================================================\n",
              "Total params: 669,706\n",
              "Trainable params: 669,706\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.67\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.01\n",
              "Params size (MB): 2.68\n",
              "Estimated Total Size (MB): 2.69\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "# Define model --> base\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "summary(model, (1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  401408\n",
            "     512\n",
            "  262144\n",
            "     512\n",
            "    5120\n",
            "      10\n",
            "________\n",
            "  669706\n"
          ]
        }
      ],
      "source": [
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "wider_model                              --                        --\n",
              "├─Flatten: 1-1                           [1, 784]                  --\n",
              "├─Sequential: 1-2                        [1, 10]                   --\n",
              "│    └─Linear: 2-1                       [1, 1024]                 803,840\n",
              "│    └─ReLU: 2-2                         [1, 1024]                 --\n",
              "│    └─Linear: 2-3                       [1, 1024]                 1,049,600\n",
              "│    └─ReLU: 2-4                         [1, 1024]                 --\n",
              "│    └─Linear: 2-5                       [1, 10]                   10,250\n",
              "==========================================================================================\n",
              "Total params: 1,863,690\n",
              "Trainable params: 1,863,690\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.86\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.02\n",
              "Params size (MB): 7.45\n",
              "Estimated Total Size (MB): 7.47\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a model --> wider\n",
        "# create a wider model\n",
        "class wider_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(wider_model, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "    \n",
        "wide_model = wider_model().to(device)\n",
        "summary(wide_model, (1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "deeper_model                             --\n",
              "├─Flatten: 1-1                           --\n",
              "├─Sequential: 1-2                        --\n",
              "│    └─Linear: 2-1                       401,920\n",
              "│    └─ReLU: 2-2                         --\n",
              "│    └─Linear: 2-3                       262,656\n",
              "│    └─ReLU: 2-4                         --\n",
              "│    └─Linear: 2-5                       5,130\n",
              "│    └─ReLU: 2-6                         --\n",
              "│    └─Linear: 2-7                       110\n",
              "=================================================================\n",
              "Total params: 669,816\n",
              "Trainable params: 669,816\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a model --> deeper\n",
        "# create a deeper model\n",
        "class deeper_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(deeper_model, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "    \n",
        "deep_model = deeper_model().to(device)\n",
        "summary(deep_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFZYEHY7ADvS"
      },
      "source": [
        "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.297300  [    0/60000]\n",
            "loss:     nan  [ 6400/60000]\n",
            "loss:     nan  [12800/60000]\n",
            "loss:     nan  [19200/60000]\n",
            "loss:     nan  [25600/60000]\n",
            "loss:     nan  [32000/60000]\n",
            "loss:     nan  [38400/60000]\n",
            "loss:     nan  [44800/60000]\n",
            "loss:     nan  [51200/60000]\n",
            "loss:     nan  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.0%, Avg loss:      nan \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and test the model\n",
        "loss_fn, optimizer = sgd_optimizer(model, lr=1)\n",
        "epochs = 100\n",
        "losses = []\n",
        "accuracies = []\n",
        "mean_losses = []\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "    \n",
        "    loss = get_loss(test_dataloader, model, loss_fn)\n",
        "    acc = get_score(test_dataloader, model, loss_fn)\n",
        "    mean_loss = get_mean_loss(test_dataloader, model, loss_fn, optimizer)\n",
        "    \n",
        "    losses.append(loss)\n",
        "    accuracies.append(acc)\n",
        "    mean_losses.append(mean_loss)\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the loss and accuracy\n",
        "for i in range(len(losses)):\n",
        "    plt.plot(i, losses[i], 'bo')\n",
        "    plt.plot(i, accuracies[i], 'ro')\n",
        "    plt.plot(i, mean_losses[i], 'go')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss/Accuracy/Mean Loss')\n",
        "    plt.legend(['Loss', 'Accuracy', 'Mean Loss'])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJLACDm9AKxv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.013553  [    0/60000]\n",
            "loss: 0.026610  [ 6400/60000]\n",
            "loss: 0.010273  [12800/60000]\n",
            "loss: 0.007788  [19200/60000]\n",
            "loss: 0.014043  [25600/60000]\n",
            "loss: 0.023430  [32000/60000]\n",
            "loss: 0.007442  [38400/60000]\n",
            "loss: 0.018125  [44800/60000]\n",
            "loss: 0.032278  [51200/60000]\n",
            "loss: 0.029111  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 91.0%, Avg loss: 0.486484 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.097522  [    0/60000]\n",
            "loss: 0.054456  [ 6400/60000]\n",
            "loss: 0.036604  [12800/60000]\n",
            "loss: 0.010813  [19200/60000]\n",
            "loss: 0.013255  [25600/60000]\n",
            "loss: 0.008319  [32000/60000]\n",
            "loss: 0.005211  [38400/60000]\n",
            "loss: 0.014376  [44800/60000]\n",
            "loss: 0.001427  [51200/60000]\n",
            "loss: 0.011864  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 91.2%, Avg loss: 0.449826 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.085897  [    0/60000]\n",
            "loss: 0.029798  [ 6400/60000]\n",
            "loss: 0.048483  [12800/60000]\n",
            "loss: 0.031619  [19200/60000]\n",
            "loss: 0.012551  [25600/60000]\n",
            "loss: 0.007207  [32000/60000]\n",
            "loss: 0.070229  [38400/60000]\n",
            "loss: 0.012756  [44800/60000]\n",
            "loss: 0.007060  [51200/60000]\n",
            "loss: 0.014839  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 91.8%, Avg loss: 0.401066 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.074669  [    0/60000]\n",
            "loss: 0.018182  [ 6400/60000]\n",
            "loss: 0.008585  [12800/60000]\n",
            "loss: 0.011434  [19200/60000]\n",
            "loss: 0.011546  [25600/60000]\n",
            "loss: 0.010794  [32000/60000]\n",
            "loss: 0.010300  [38400/60000]\n",
            "loss: 0.013023  [44800/60000]\n",
            "loss: 0.005891  [51200/60000]\n",
            "loss: 0.019368  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 92.0%, Avg loss: 0.403971 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.068048  [    0/60000]\n",
            "loss: 0.017683  [ 6400/60000]\n",
            "loss: 0.010478  [12800/60000]\n",
            "loss: 0.043939  [19200/60000]\n",
            "loss: 0.011567  [25600/60000]\n",
            "loss: 0.015417  [32000/60000]\n",
            "loss: 0.010865  [38400/60000]\n",
            "loss: 0.017347  [44800/60000]\n",
            "loss: 0.003181  [51200/60000]\n",
            "loss: 0.020893  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 92.5%, Avg loss: 0.355890 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.058590  [    0/60000]\n",
            "loss: 0.020908  [ 6400/60000]\n",
            "loss: 0.027544  [12800/60000]\n",
            "loss: 0.032295  [19200/60000]\n",
            "loss: 0.007696  [25600/60000]\n",
            "loss: 0.006085  [32000/60000]\n",
            "loss: 0.012924  [38400/60000]\n",
            "loss: 0.016078  [44800/60000]\n",
            "loss: 0.005076  [51200/60000]\n",
            "loss: 0.008196  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 92.4%, Avg loss: 0.378883 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.054454  [    0/60000]\n",
            "loss: 0.029329  [ 6400/60000]\n",
            "loss: 0.006497  [12800/60000]\n",
            "loss: 0.027828  [19200/60000]\n",
            "loss: 0.009552  [25600/60000]\n",
            "loss: 0.048629  [32000/60000]\n",
            "loss: 0.004480  [38400/60000]\n",
            "loss: 0.037432  [44800/60000]\n",
            "loss: 0.002894  [51200/60000]\n",
            "loss: 0.004601  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 92.5%, Avg loss: 0.347906 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.081052  [    0/60000]\n",
            "loss: 0.025923  [ 6400/60000]\n",
            "loss: 0.005927  [12800/60000]\n",
            "loss: 0.006448  [19200/60000]\n",
            "loss: 0.043065  [25600/60000]\n",
            "loss: 0.007397  [32000/60000]\n",
            "loss: 0.003769  [38400/60000]\n",
            "loss: 0.010231  [44800/60000]\n",
            "loss: 0.026155  [51200/60000]\n",
            "loss: 0.014911  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.0%, Avg loss: 0.346171 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.058376  [    0/60000]\n",
            "loss: 0.021131  [ 6400/60000]\n",
            "loss: 0.004724  [12800/60000]\n",
            "loss: 0.003061  [19200/60000]\n",
            "loss: 0.029022  [25600/60000]\n",
            "loss: 0.008538  [32000/60000]\n",
            "loss: 0.033952  [38400/60000]\n",
            "loss: 0.008261  [44800/60000]\n",
            "loss: 0.006853  [51200/60000]\n",
            "loss: 0.005436  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 92.0%, Avg loss: 0.365813 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.066427  [    0/60000]\n",
            "loss: 0.012252  [ 6400/60000]\n",
            "loss: 0.026028  [12800/60000]\n",
            "loss: 0.026938  [19200/60000]\n",
            "loss: 0.015375  [25600/60000]\n",
            "loss: 0.042753  [32000/60000]\n",
            "loss: 0.020856  [38400/60000]\n",
            "loss: 0.008705  [44800/60000]\n",
            "loss: 0.011306  [51200/60000]\n",
            "loss: 0.015756  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.2%, Avg loss: 0.309957 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.048538  [    0/60000]\n",
            "loss: 0.010833  [ 6400/60000]\n",
            "loss: 0.004482  [12800/60000]\n",
            "loss: 0.026225  [19200/60000]\n",
            "loss: 0.004983  [25600/60000]\n",
            "loss: 0.014134  [32000/60000]\n",
            "loss: 0.008097  [38400/60000]\n",
            "loss: 0.011868  [44800/60000]\n",
            "loss: 0.001472  [51200/60000]\n",
            "loss: 0.027829  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.2%, Avg loss: 0.322400 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.046303  [    0/60000]\n",
            "loss: 0.018261  [ 6400/60000]\n",
            "loss: 0.006406  [12800/60000]\n",
            "loss: 0.002421  [19200/60000]\n",
            "loss: 0.001318  [25600/60000]\n",
            "loss: 0.005819  [32000/60000]\n",
            "loss: 0.003187  [38400/60000]\n",
            "loss: 0.059634  [44800/60000]\n",
            "loss: 0.004655  [51200/60000]\n",
            "loss: 0.014408  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.0%, Avg loss: 0.322231 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.041636  [    0/60000]\n",
            "loss: 0.022854  [ 6400/60000]\n",
            "loss: 0.004690  [12800/60000]\n",
            "loss: 0.012890  [19200/60000]\n",
            "loss: 0.005927  [25600/60000]\n",
            "loss: 0.114717  [32000/60000]\n",
            "loss: 0.008044  [38400/60000]\n",
            "loss: 0.010819  [44800/60000]\n",
            "loss: 0.010822  [51200/60000]\n",
            "loss: 0.013038  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.2%, Avg loss: 0.304897 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.045225  [    0/60000]\n",
            "loss: 0.020525  [ 6400/60000]\n",
            "loss: 0.015166  [12800/60000]\n",
            "loss: 0.010880  [19200/60000]\n",
            "loss: 0.010893  [25600/60000]\n",
            "loss: 0.014234  [32000/60000]\n",
            "loss: 0.017048  [38400/60000]\n",
            "loss: 0.000653  [44800/60000]\n",
            "loss: 0.006025  [51200/60000]\n",
            "loss: 0.016027  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.2%, Avg loss: 0.302485 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.055073  [    0/60000]\n",
            "loss: 0.026961  [ 6400/60000]\n",
            "loss: 0.008683  [12800/60000]\n",
            "loss: 0.012662  [19200/60000]\n",
            "loss: 0.003332  [25600/60000]\n",
            "loss: 0.008696  [32000/60000]\n",
            "loss: 0.006998  [38400/60000]\n",
            "loss: 0.014901  [44800/60000]\n",
            "loss: 0.096068  [51200/60000]\n",
            "loss: 0.006739  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.4%, Avg loss: 0.301871 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.032985  [    0/60000]\n",
            "loss: 0.022294  [ 6400/60000]\n",
            "loss: 0.004522  [12800/60000]\n",
            "loss: 0.003862  [19200/60000]\n",
            "loss: 0.002650  [25600/60000]\n",
            "loss: 0.011064  [32000/60000]\n",
            "loss: 0.022378  [38400/60000]\n",
            "loss: 0.003503  [44800/60000]\n",
            "loss: 0.002552  [51200/60000]\n",
            "loss: 0.044175  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.8%, Avg loss: 0.278172 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.031172  [    0/60000]\n",
            "loss: 0.019507  [ 6400/60000]\n",
            "loss: 0.002638  [12800/60000]\n",
            "loss: 0.005781  [19200/60000]\n",
            "loss: 0.056356  [25600/60000]\n",
            "loss: 0.008471  [32000/60000]\n",
            "loss: 0.048384  [38400/60000]\n",
            "loss: 0.029796  [44800/60000]\n",
            "loss: 0.007880  [51200/60000]\n",
            "loss: 0.009946  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.3%, Avg loss: 0.327115 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.026819  [    0/60000]\n",
            "loss: 0.023993  [ 6400/60000]\n",
            "loss: 0.063416  [12800/60000]\n",
            "loss: 0.003339  [19200/60000]\n",
            "loss: 0.003811  [25600/60000]\n",
            "loss: 0.015234  [32000/60000]\n",
            "loss: 0.008752  [38400/60000]\n",
            "loss: 0.003380  [44800/60000]\n",
            "loss: 0.005408  [51200/60000]\n",
            "loss: 0.090202  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.5%, Avg loss: 0.300147 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.029953  [    0/60000]\n",
            "loss: 0.019653  [ 6400/60000]\n",
            "loss: 0.023210  [12800/60000]\n",
            "loss: 0.007534  [19200/60000]\n",
            "loss: 0.005263  [25600/60000]\n",
            "loss: 0.035265  [32000/60000]\n",
            "loss: 0.052482  [38400/60000]\n",
            "loss: 0.006843  [44800/60000]\n",
            "loss: 0.006374  [51200/60000]\n",
            "loss: 0.005777  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.3%, Avg loss: 0.270823 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.038478  [    0/60000]\n",
            "loss: 0.020577  [ 6400/60000]\n",
            "loss: 0.001508  [12800/60000]\n",
            "loss: 0.035277  [19200/60000]\n",
            "loss: 0.001434  [25600/60000]\n",
            "loss: 0.082600  [32000/60000]\n",
            "loss: 0.003124  [38400/60000]\n",
            "loss: 0.012107  [44800/60000]\n",
            "loss: 0.028519  [51200/60000]\n",
            "loss: 0.002858  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.0%, Avg loss: 0.265287 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.030731  [    0/60000]\n",
            "loss: 0.019985  [ 6400/60000]\n",
            "loss: 0.016051  [12800/60000]\n",
            "loss: 0.002916  [19200/60000]\n",
            "loss: 0.004206  [25600/60000]\n",
            "loss: 0.018190  [32000/60000]\n",
            "loss: 0.001663  [38400/60000]\n",
            "loss: 0.001785  [44800/60000]\n",
            "loss: 0.005096  [51200/60000]\n",
            "loss: 0.021409  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.7%, Avg loss: 0.276438 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.019047  [    0/60000]\n",
            "loss: 0.016741  [ 6400/60000]\n",
            "loss: 0.008879  [12800/60000]\n",
            "loss: 0.009431  [19200/60000]\n",
            "loss: 0.013486  [25600/60000]\n",
            "loss: 0.011777  [32000/60000]\n",
            "loss: 0.001390  [38400/60000]\n",
            "loss: 0.009460  [44800/60000]\n",
            "loss: 0.002967  [51200/60000]\n",
            "loss: 0.004324  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.3%, Avg loss: 0.253104 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.027742  [    0/60000]\n",
            "loss: 0.016444  [ 6400/60000]\n",
            "loss: 0.000943  [12800/60000]\n",
            "loss: 0.002513  [19200/60000]\n",
            "loss: 0.005380  [25600/60000]\n",
            "loss: 0.029529  [32000/60000]\n",
            "loss: 0.000629  [38400/60000]\n",
            "loss: 0.024598  [44800/60000]\n",
            "loss: 0.002200  [51200/60000]\n",
            "loss: 0.011348  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.8%, Avg loss: 0.292859 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.025582  [    0/60000]\n",
            "loss: 0.059721  [ 6400/60000]\n",
            "loss: 0.002732  [12800/60000]\n",
            "loss: 0.003131  [19200/60000]\n",
            "loss: 0.043752  [25600/60000]\n",
            "loss: 0.027843  [32000/60000]\n",
            "loss: 0.037934  [38400/60000]\n",
            "loss: 0.009151  [44800/60000]\n",
            "loss: 0.017775  [51200/60000]\n",
            "loss: 0.026214  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.5%, Avg loss: 0.292081 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.028260  [    0/60000]\n",
            "loss: 0.011272  [ 6400/60000]\n",
            "loss: 0.011433  [12800/60000]\n",
            "loss: 0.103098  [19200/60000]\n",
            "loss: 0.009638  [25600/60000]\n",
            "loss: 0.037159  [32000/60000]\n",
            "loss: 0.023890  [38400/60000]\n",
            "loss: 0.018811  [44800/60000]\n",
            "loss: 0.003598  [51200/60000]\n",
            "loss: 0.010277  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.6%, Avg loss: 0.300307 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.027309  [    0/60000]\n",
            "loss: 0.027812  [ 6400/60000]\n",
            "loss: 0.004819  [12800/60000]\n",
            "loss: 0.030627  [19200/60000]\n",
            "loss: 0.024026  [25600/60000]\n",
            "loss: 0.013951  [32000/60000]\n",
            "loss: 0.005415  [38400/60000]\n",
            "loss: 0.004574  [44800/60000]\n",
            "loss: 0.000448  [51200/60000]\n",
            "loss: 0.015306  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.5%, Avg loss: 0.248786 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.011912  [    0/60000]\n",
            "loss: 0.022666  [ 6400/60000]\n",
            "loss: 0.025130  [12800/60000]\n",
            "loss: 0.003562  [19200/60000]\n",
            "loss: 0.004750  [25600/60000]\n",
            "loss: 0.096174  [32000/60000]\n",
            "loss: 0.095745  [38400/60000]\n",
            "loss: 0.001697  [44800/60000]\n",
            "loss: 0.005878  [51200/60000]\n",
            "loss: 0.039069  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.5%, Avg loss: 0.240511 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.013332  [    0/60000]\n",
            "loss: 0.011076  [ 6400/60000]\n",
            "loss: 0.002270  [12800/60000]\n",
            "loss: 0.005451  [19200/60000]\n",
            "loss: 0.026068  [25600/60000]\n",
            "loss: 0.011734  [32000/60000]\n",
            "loss: 0.001429  [38400/60000]\n",
            "loss: 0.027634  [44800/60000]\n",
            "loss: 0.005430  [51200/60000]\n",
            "loss: 0.026848  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.5%, Avg loss: 0.240248 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.021589  [    0/60000]\n",
            "loss: 0.013091  [ 6400/60000]\n",
            "loss: 0.003782  [12800/60000]\n",
            "loss: 0.005076  [19200/60000]\n",
            "loss: 0.005979  [25600/60000]\n",
            "loss: 0.058942  [32000/60000]\n",
            "loss: 0.003562  [38400/60000]\n",
            "loss: 0.002167  [44800/60000]\n",
            "loss: 0.002796  [51200/60000]\n",
            "loss: 0.017185  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.7%, Avg loss: 0.236189 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.011679  [    0/60000]\n",
            "loss: 0.009877  [ 6400/60000]\n",
            "loss: 0.004484  [12800/60000]\n",
            "loss: 0.007066  [19200/60000]\n",
            "loss: 0.006565  [25600/60000]\n",
            "loss: 0.014995  [32000/60000]\n",
            "loss: 0.002510  [38400/60000]\n",
            "loss: 0.000481  [44800/60000]\n",
            "loss: 0.003563  [51200/60000]\n",
            "loss: 0.003120  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.3%, Avg loss: 0.263054 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.014189  [    0/60000]\n",
            "loss: 0.007912  [ 6400/60000]\n",
            "loss: 0.003434  [12800/60000]\n",
            "loss: 0.011218  [19200/60000]\n",
            "loss: 0.003244  [25600/60000]\n",
            "loss: 0.022267  [32000/60000]\n",
            "loss: 0.004107  [38400/60000]\n",
            "loss: 0.002899  [44800/60000]\n",
            "loss: 0.011599  [51200/60000]\n",
            "loss: 0.026330  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.9%, Avg loss: 0.266157 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.008568  [    0/60000]\n",
            "loss: 0.015033  [ 6400/60000]\n",
            "loss: 0.002188  [12800/60000]\n",
            "loss: 0.028558  [19200/60000]\n",
            "loss: 0.003440  [25600/60000]\n",
            "loss: 0.012953  [32000/60000]\n",
            "loss: 0.002299  [38400/60000]\n",
            "loss: 0.019493  [44800/60000]\n",
            "loss: 0.001262  [51200/60000]\n",
            "loss: 0.013193  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.1%, Avg loss: 0.211072 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.010723  [    0/60000]\n",
            "loss: 0.012713  [ 6400/60000]\n",
            "loss: 0.003013  [12800/60000]\n",
            "loss: 0.002418  [19200/60000]\n",
            "loss: 0.005831  [25600/60000]\n",
            "loss: 0.004968  [32000/60000]\n",
            "loss: 0.005516  [38400/60000]\n",
            "loss: 0.047469  [44800/60000]\n",
            "loss: 0.001023  [51200/60000]\n",
            "loss: 0.008589  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 93.0%, Avg loss: 0.334725 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.008156  [    0/60000]\n",
            "loss: 0.007133  [ 6400/60000]\n",
            "loss: 0.002798  [12800/60000]\n",
            "loss: 0.049883  [19200/60000]\n",
            "loss: 0.060235  [25600/60000]\n",
            "loss: 0.013214  [32000/60000]\n",
            "loss: 0.004368  [38400/60000]\n",
            "loss: 0.024478  [44800/60000]\n",
            "loss: 0.003969  [51200/60000]\n",
            "loss: 0.023642  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.228268 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.010730  [    0/60000]\n",
            "loss: 0.006736  [ 6400/60000]\n",
            "loss: 0.001412  [12800/60000]\n",
            "loss: 0.001314  [19200/60000]\n",
            "loss: 0.002455  [25600/60000]\n",
            "loss: 0.007555  [32000/60000]\n",
            "loss: 0.004864  [38400/60000]\n",
            "loss: 0.008188  [44800/60000]\n",
            "loss: 0.010595  [51200/60000]\n",
            "loss: 0.010456  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.0%, Avg loss: 0.222392 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.008051  [    0/60000]\n",
            "loss: 0.017018  [ 6400/60000]\n",
            "loss: 0.000851  [12800/60000]\n",
            "loss: 0.009567  [19200/60000]\n",
            "loss: 0.001547  [25600/60000]\n",
            "loss: 0.001737  [32000/60000]\n",
            "loss: 0.015852  [38400/60000]\n",
            "loss: 0.000688  [44800/60000]\n",
            "loss: 0.004845  [51200/60000]\n",
            "loss: 0.004286  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.5%, Avg loss: 0.212705 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.008878  [    0/60000]\n",
            "loss: 0.007654  [ 6400/60000]\n",
            "loss: 0.003801  [12800/60000]\n",
            "loss: 0.004735  [19200/60000]\n",
            "loss: 0.017921  [25600/60000]\n",
            "loss: 0.008375  [32000/60000]\n",
            "loss: 0.009405  [38400/60000]\n",
            "loss: 0.000787  [44800/60000]\n",
            "loss: 0.012225  [51200/60000]\n",
            "loss: 0.018422  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.2%, Avg loss: 0.212035 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.007167  [    0/60000]\n",
            "loss: 0.004555  [ 6400/60000]\n",
            "loss: 0.115817  [12800/60000]\n",
            "loss: 0.000814  [19200/60000]\n",
            "loss: 0.065952  [25600/60000]\n",
            "loss: 0.008450  [32000/60000]\n",
            "loss: 0.003909  [38400/60000]\n",
            "loss: 0.002114  [44800/60000]\n",
            "loss: 0.007514  [51200/60000]\n",
            "loss: 0.030362  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.6%, Avg loss: 0.189088 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.012433  [    0/60000]\n",
            "loss: 0.008226  [ 6400/60000]\n",
            "loss: 0.000278  [12800/60000]\n",
            "loss: 0.001571  [19200/60000]\n",
            "loss: 0.005020  [25600/60000]\n",
            "loss: 0.102485  [32000/60000]\n",
            "loss: 0.001207  [38400/60000]\n",
            "loss: 0.000588  [44800/60000]\n",
            "loss: 0.001056  [51200/60000]\n",
            "loss: 0.033540  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.8%, Avg loss: 0.202905 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.004620  [    0/60000]\n",
            "loss: 0.006043  [ 6400/60000]\n",
            "loss: 0.000566  [12800/60000]\n",
            "loss: 0.009448  [19200/60000]\n",
            "loss: 0.033203  [25600/60000]\n",
            "loss: 0.004438  [32000/60000]\n",
            "loss: 0.000993  [38400/60000]\n",
            "loss: 0.008900  [44800/60000]\n",
            "loss: 0.026884  [51200/60000]\n",
            "loss: 0.051399  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.207348 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.004909  [    0/60000]\n",
            "loss: 0.010122  [ 6400/60000]\n",
            "loss: 0.007573  [12800/60000]\n",
            "loss: 0.002655  [19200/60000]\n",
            "loss: 0.001569  [25600/60000]\n",
            "loss: 0.001677  [32000/60000]\n",
            "loss: 0.120723  [38400/60000]\n",
            "loss: 0.012061  [44800/60000]\n",
            "loss: 0.001748  [51200/60000]\n",
            "loss: 0.012236  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.3%, Avg loss: 0.221549 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.004547  [    0/60000]\n",
            "loss: 0.008191  [ 6400/60000]\n",
            "loss: 0.001980  [12800/60000]\n",
            "loss: 0.004780  [19200/60000]\n",
            "loss: 0.014804  [25600/60000]\n",
            "loss: 0.000685  [32000/60000]\n",
            "loss: 0.005998  [38400/60000]\n",
            "loss: 0.000530  [44800/60000]\n",
            "loss: 0.050308  [51200/60000]\n",
            "loss: 0.032784  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.2%, Avg loss: 0.216319 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.004709  [    0/60000]\n",
            "loss: 0.010034  [ 6400/60000]\n",
            "loss: 0.005406  [12800/60000]\n",
            "loss: 0.001262  [19200/60000]\n",
            "loss: 0.010816  [25600/60000]\n",
            "loss: 0.054692  [32000/60000]\n",
            "loss: 0.004596  [38400/60000]\n",
            "loss: 0.000201  [44800/60000]\n",
            "loss: 0.001186  [51200/60000]\n",
            "loss: 0.009468  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.5%, Avg loss: 0.261173 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.013412  [    0/60000]\n",
            "loss: 0.017924  [ 6400/60000]\n",
            "loss: 0.006737  [12800/60000]\n",
            "loss: 0.001309  [19200/60000]\n",
            "loss: 0.009346  [25600/60000]\n",
            "loss: 0.021813  [32000/60000]\n",
            "loss: 0.003014  [38400/60000]\n",
            "loss: 0.090848  [44800/60000]\n",
            "loss: 0.009807  [51200/60000]\n",
            "loss: 0.120479  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.2%, Avg loss: 0.208671 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.005101  [    0/60000]\n",
            "loss: 0.010197  [ 6400/60000]\n",
            "loss: 0.007214  [12800/60000]\n",
            "loss: 0.005617  [19200/60000]\n",
            "loss: 0.002000  [25600/60000]\n",
            "loss: 0.003990  [32000/60000]\n",
            "loss: 0.002221  [38400/60000]\n",
            "loss: 0.001026  [44800/60000]\n",
            "loss: 0.009160  [51200/60000]\n",
            "loss: 0.129160  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.222817 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.008738  [    0/60000]\n",
            "loss: 0.005034  [ 6400/60000]\n",
            "loss: 0.017208  [12800/60000]\n",
            "loss: 0.005863  [19200/60000]\n",
            "loss: 0.040155  [25600/60000]\n",
            "loss: 0.007053  [32000/60000]\n",
            "loss: 0.007936  [38400/60000]\n",
            "loss: 0.019456  [44800/60000]\n",
            "loss: 0.002304  [51200/60000]\n",
            "loss: 0.020124  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.3%, Avg loss: 0.211283 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.003483  [    0/60000]\n",
            "loss: 0.010839  [ 6400/60000]\n",
            "loss: 0.002117  [12800/60000]\n",
            "loss: 0.023389  [19200/60000]\n",
            "loss: 0.000624  [25600/60000]\n",
            "loss: 0.004436  [32000/60000]\n",
            "loss: 0.000616  [38400/60000]\n",
            "loss: 0.000119  [44800/60000]\n",
            "loss: 0.001942  [51200/60000]\n",
            "loss: 0.006830  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.197339 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.006138  [    0/60000]\n",
            "loss: 0.009897  [ 6400/60000]\n",
            "loss: 0.000669  [12800/60000]\n",
            "loss: 0.079802  [19200/60000]\n",
            "loss: 0.035542  [25600/60000]\n",
            "loss: 0.042289  [32000/60000]\n",
            "loss: 0.072157  [38400/60000]\n",
            "loss: 0.006672  [44800/60000]\n",
            "loss: 0.002024  [51200/60000]\n",
            "loss: 0.006476  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.9%, Avg loss: 0.233078 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.015801  [    0/60000]\n",
            "loss: 0.009474  [ 6400/60000]\n",
            "loss: 0.004845  [12800/60000]\n",
            "loss: 0.001070  [19200/60000]\n",
            "loss: 0.036048  [25600/60000]\n",
            "loss: 0.023890  [32000/60000]\n",
            "loss: 0.010848  [38400/60000]\n",
            "loss: 0.012504  [44800/60000]\n",
            "loss: 0.005756  [51200/60000]\n",
            "loss: 0.027040  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.4%, Avg loss: 0.212706 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.012843  [    0/60000]\n",
            "loss: 0.022458  [ 6400/60000]\n",
            "loss: 0.007891  [12800/60000]\n",
            "loss: 0.005511  [19200/60000]\n",
            "loss: 0.010223  [25600/60000]\n",
            "loss: 0.018755  [32000/60000]\n",
            "loss: 0.002773  [38400/60000]\n",
            "loss: 0.012243  [44800/60000]\n",
            "loss: 0.003826  [51200/60000]\n",
            "loss: 0.006767  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.234410 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.006184  [    0/60000]\n",
            "loss: 0.010377  [ 6400/60000]\n",
            "loss: 0.006334  [12800/60000]\n",
            "loss: 0.016802  [19200/60000]\n",
            "loss: 0.009425  [25600/60000]\n",
            "loss: 0.008323  [32000/60000]\n",
            "loss: 0.001031  [38400/60000]\n",
            "loss: 0.001504  [44800/60000]\n",
            "loss: 0.002464  [51200/60000]\n",
            "loss: 0.005904  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.3%, Avg loss: 0.159956 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.018675  [    0/60000]\n",
            "loss: 0.004290  [ 6400/60000]\n",
            "loss: 0.005688  [12800/60000]\n",
            "loss: 0.008342  [19200/60000]\n",
            "loss: 0.005551  [25600/60000]\n",
            "loss: 0.000547  [32000/60000]\n",
            "loss: 0.008110  [38400/60000]\n",
            "loss: 0.056008  [44800/60000]\n",
            "loss: 0.001356  [51200/60000]\n",
            "loss: 0.005987  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.6%, Avg loss: 0.210591 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.017260  [    0/60000]\n",
            "loss: 0.009828  [ 6400/60000]\n",
            "loss: 0.000863  [12800/60000]\n",
            "loss: 0.003631  [19200/60000]\n",
            "loss: 0.002313  [25600/60000]\n",
            "loss: 0.005538  [32000/60000]\n",
            "loss: 0.006843  [38400/60000]\n",
            "loss: 0.000449  [44800/60000]\n",
            "loss: 0.002725  [51200/60000]\n",
            "loss: 0.013277  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.8%, Avg loss: 0.194345 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.003417  [    0/60000]\n",
            "loss: 0.004482  [ 6400/60000]\n",
            "loss: 0.004531  [12800/60000]\n",
            "loss: 0.000741  [19200/60000]\n",
            "loss: 0.001769  [25600/60000]\n",
            "loss: 0.003882  [32000/60000]\n",
            "loss: 0.007836  [38400/60000]\n",
            "loss: 0.011481  [44800/60000]\n",
            "loss: 0.001441  [51200/60000]\n",
            "loss: 0.009526  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.5%, Avg loss: 0.206745 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.004861  [    0/60000]\n",
            "loss: 0.005866  [ 6400/60000]\n",
            "loss: 0.001740  [12800/60000]\n",
            "loss: 0.001805  [19200/60000]\n",
            "loss: 0.011197  [25600/60000]\n",
            "loss: 0.001919  [32000/60000]\n",
            "loss: 0.018629  [38400/60000]\n",
            "loss: 0.000622  [44800/60000]\n",
            "loss: 0.004647  [51200/60000]\n",
            "loss: 0.045962  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.3%, Avg loss: 0.169916 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.006917  [    0/60000]\n",
            "loss: 0.006095  [ 6400/60000]\n",
            "loss: 0.003534  [12800/60000]\n",
            "loss: 0.002087  [19200/60000]\n",
            "loss: 0.001740  [25600/60000]\n",
            "loss: 0.007230  [32000/60000]\n",
            "loss: 0.012843  [38400/60000]\n",
            "loss: 0.000779  [44800/60000]\n",
            "loss: 0.000432  [51200/60000]\n",
            "loss: 0.052280  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.9%, Avg loss: 0.170414 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.003520  [    0/60000]\n",
            "loss: 0.006645  [ 6400/60000]\n",
            "loss: 0.126562  [12800/60000]\n",
            "loss: 0.005882  [19200/60000]\n",
            "loss: 0.041125  [25600/60000]\n",
            "loss: 0.019898  [32000/60000]\n",
            "loss: 0.002356  [38400/60000]\n",
            "loss: 0.008878  [44800/60000]\n",
            "loss: 0.048338  [51200/60000]\n",
            "loss: 0.012780  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.3%, Avg loss: 0.262487 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.004874  [    0/60000]\n",
            "loss: 0.012948  [ 6400/60000]\n",
            "loss: 0.035242  [12800/60000]\n",
            "loss: 0.001304  [19200/60000]\n",
            "loss: 0.014487  [25600/60000]\n",
            "loss: 0.015459  [32000/60000]\n",
            "loss: 0.007168  [38400/60000]\n",
            "loss: 0.001347  [44800/60000]\n",
            "loss: 0.009959  [51200/60000]\n",
            "loss: 0.005872  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.0%, Avg loss: 0.294636 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.006782  [    0/60000]\n",
            "loss: 0.011115  [ 6400/60000]\n",
            "loss: 0.000676  [12800/60000]\n",
            "loss: 0.006097  [19200/60000]\n",
            "loss: 0.000312  [25600/60000]\n",
            "loss: 0.002349  [32000/60000]\n",
            "loss: 0.007942  [38400/60000]\n",
            "loss: 0.001802  [44800/60000]\n",
            "loss: 0.002393  [51200/60000]\n",
            "loss: 0.017792  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.2%, Avg loss: 0.262284 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.005915  [    0/60000]\n",
            "loss: 0.015505  [ 6400/60000]\n",
            "loss: 0.007801  [12800/60000]\n",
            "loss: 0.017232  [19200/60000]\n",
            "loss: 0.013639  [25600/60000]\n",
            "loss: 0.001287  [32000/60000]\n",
            "loss: 0.001179  [38400/60000]\n",
            "loss: 0.002462  [44800/60000]\n",
            "loss: 0.004255  [51200/60000]\n",
            "loss: 0.015022  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.2%, Avg loss: 0.158255 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.007989  [    0/60000]\n",
            "loss: 0.008931  [ 6400/60000]\n",
            "loss: 0.003474  [12800/60000]\n",
            "loss: 0.005326  [19200/60000]\n",
            "loss: 0.017437  [25600/60000]\n",
            "loss: 0.004307  [32000/60000]\n",
            "loss: 0.042944  [38400/60000]\n",
            "loss: 0.031993  [44800/60000]\n",
            "loss: 0.013560  [51200/60000]\n",
            "loss: 0.011906  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.154928 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.017830  [    0/60000]\n",
            "loss: 0.009085  [ 6400/60000]\n",
            "loss: 0.000689  [12800/60000]\n",
            "loss: 0.004804  [19200/60000]\n",
            "loss: 0.018809  [25600/60000]\n",
            "loss: 0.003579  [32000/60000]\n",
            "loss: 0.001644  [38400/60000]\n",
            "loss: 0.000939  [44800/60000]\n",
            "loss: 0.020032  [51200/60000]\n",
            "loss: 0.002202  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.8%, Avg loss: 0.137529 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.004638  [    0/60000]\n",
            "loss: 0.008041  [ 6400/60000]\n",
            "loss: 0.000877  [12800/60000]\n",
            "loss: 0.003956  [19200/60000]\n",
            "loss: 0.000252  [25600/60000]\n",
            "loss: 0.005142  [32000/60000]\n",
            "loss: 0.000683  [38400/60000]\n",
            "loss: 0.000585  [44800/60000]\n",
            "loss: 0.003621  [51200/60000]\n",
            "loss: 0.009493  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.4%, Avg loss: 0.153349 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.006635  [    0/60000]\n",
            "loss: 0.005225  [ 6400/60000]\n",
            "loss: 0.000478  [12800/60000]\n",
            "loss: 0.002083  [19200/60000]\n",
            "loss: 0.002380  [25600/60000]\n",
            "loss: 0.003516  [32000/60000]\n",
            "loss: 0.002943  [38400/60000]\n",
            "loss: 0.000315  [44800/60000]\n",
            "loss: 0.006628  [51200/60000]\n",
            "loss: 0.004209  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.8%, Avg loss: 0.138372 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.001318  [    0/60000]\n",
            "loss: 0.004318  [ 6400/60000]\n",
            "loss: 0.001935  [12800/60000]\n",
            "loss: 0.003143  [19200/60000]\n",
            "loss: 0.004737  [25600/60000]\n",
            "loss: 0.001899  [32000/60000]\n",
            "loss: 0.008585  [38400/60000]\n",
            "loss: 0.000140  [44800/60000]\n",
            "loss: 0.046834  [51200/60000]\n",
            "loss: 0.002506  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.3%, Avg loss: 0.163445 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.002512  [    0/60000]\n",
            "loss: 0.004978  [ 6400/60000]\n",
            "loss: 0.002595  [12800/60000]\n",
            "loss: 0.000698  [19200/60000]\n",
            "loss: 0.001426  [25600/60000]\n",
            "loss: 0.002164  [32000/60000]\n",
            "loss: 0.001825  [38400/60000]\n",
            "loss: 0.000771  [44800/60000]\n",
            "loss: 0.079161  [51200/60000]\n",
            "loss: 0.031844  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.3%, Avg loss: 0.158997 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.004519  [    0/60000]\n",
            "loss: 0.007411  [ 6400/60000]\n",
            "loss: 0.017173  [12800/60000]\n",
            "loss: 0.002810  [19200/60000]\n",
            "loss: 0.001343  [25600/60000]\n",
            "loss: 0.006616  [32000/60000]\n",
            "loss: 0.001121  [38400/60000]\n",
            "loss: 0.000826  [44800/60000]\n",
            "loss: 0.002531  [51200/60000]\n",
            "loss: 0.002064  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.152621 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.009843  [    0/60000]\n",
            "loss: 0.005358  [ 6400/60000]\n",
            "loss: 0.002123  [12800/60000]\n",
            "loss: 0.002784  [19200/60000]\n",
            "loss: 0.002877  [25600/60000]\n",
            "loss: 0.000754  [32000/60000]\n",
            "loss: 0.002001  [38400/60000]\n",
            "loss: 0.197158  [44800/60000]\n",
            "loss: 0.000897  [51200/60000]\n",
            "loss: 0.026551  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.126206 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.005003  [    0/60000]\n",
            "loss: 0.009605  [ 6400/60000]\n",
            "loss: 0.001824  [12800/60000]\n",
            "loss: 0.009304  [19200/60000]\n",
            "loss: 0.000617  [25600/60000]\n",
            "loss: 0.023621  [32000/60000]\n",
            "loss: 0.000952  [38400/60000]\n",
            "loss: 0.000112  [44800/60000]\n",
            "loss: 0.001156  [51200/60000]\n",
            "loss: 0.004170  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.8%, Avg loss: 0.149512 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.002656  [    0/60000]\n",
            "loss: 0.006658  [ 6400/60000]\n",
            "loss: 0.002165  [12800/60000]\n",
            "loss: 0.000548  [19200/60000]\n",
            "loss: 0.010141  [25600/60000]\n",
            "loss: 0.000858  [32000/60000]\n",
            "loss: 0.004018  [38400/60000]\n",
            "loss: 0.000165  [44800/60000]\n",
            "loss: 0.000339  [51200/60000]\n",
            "loss: 0.001512  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.9%, Avg loss: 0.126651 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.002215  [    0/60000]\n",
            "loss: 0.002543  [ 6400/60000]\n",
            "loss: 0.002589  [12800/60000]\n",
            "loss: 0.001240  [19200/60000]\n",
            "loss: 0.001204  [25600/60000]\n",
            "loss: 0.003162  [32000/60000]\n",
            "loss: 0.000738  [38400/60000]\n",
            "loss: 0.000043  [44800/60000]\n",
            "loss: 0.056481  [51200/60000]\n",
            "loss: 0.012084  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.5%, Avg loss: 0.203997 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.020910  [    0/60000]\n",
            "loss: 0.006099  [ 6400/60000]\n",
            "loss: 0.000978  [12800/60000]\n",
            "loss: 0.000819  [19200/60000]\n",
            "loss: 0.008680  [25600/60000]\n",
            "loss: 0.012568  [32000/60000]\n",
            "loss: 0.012612  [38400/60000]\n",
            "loss: 0.001397  [44800/60000]\n",
            "loss: 0.005538  [51200/60000]\n",
            "loss: 0.001532  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.143754 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.009816  [    0/60000]\n",
            "loss: 0.004378  [ 6400/60000]\n",
            "loss: 0.008012  [12800/60000]\n",
            "loss: 0.003314  [19200/60000]\n",
            "loss: 0.014817  [25600/60000]\n",
            "loss: 0.004847  [32000/60000]\n",
            "loss: 0.006839  [38400/60000]\n",
            "loss: 0.005984  [44800/60000]\n",
            "loss: 0.000525  [51200/60000]\n",
            "loss: 0.002215  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.151613 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.000603  [    0/60000]\n",
            "loss: 0.006463  [ 6400/60000]\n",
            "loss: 0.000931  [12800/60000]\n",
            "loss: 0.001353  [19200/60000]\n",
            "loss: 0.002026  [25600/60000]\n",
            "loss: 0.001799  [32000/60000]\n",
            "loss: 0.011365  [38400/60000]\n",
            "loss: 0.015379  [44800/60000]\n",
            "loss: 0.002311  [51200/60000]\n",
            "loss: 0.001050  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.111408 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.001374  [    0/60000]\n",
            "loss: 0.004710  [ 6400/60000]\n",
            "loss: 0.001402  [12800/60000]\n",
            "loss: 0.004356  [19200/60000]\n",
            "loss: 0.000504  [25600/60000]\n",
            "loss: 0.001187  [32000/60000]\n",
            "loss: 0.001764  [38400/60000]\n",
            "loss: 0.000786  [44800/60000]\n",
            "loss: 0.000207  [51200/60000]\n",
            "loss: 0.000374  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.109341 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.003116  [    0/60000]\n",
            "loss: 0.003330  [ 6400/60000]\n",
            "loss: 0.001018  [12800/60000]\n",
            "loss: 0.001231  [19200/60000]\n",
            "loss: 0.000605  [25600/60000]\n",
            "loss: 0.007434  [32000/60000]\n",
            "loss: 0.002489  [38400/60000]\n",
            "loss: 0.000444  [44800/60000]\n",
            "loss: 0.004546  [51200/60000]\n",
            "loss: 0.003261  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.102682 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.001735  [    0/60000]\n",
            "loss: 0.003408  [ 6400/60000]\n",
            "loss: 0.000749  [12800/60000]\n",
            "loss: 0.006509  [19200/60000]\n",
            "loss: 0.001534  [25600/60000]\n",
            "loss: 0.000588  [32000/60000]\n",
            "loss: 0.003000  [38400/60000]\n",
            "loss: 0.000504  [44800/60000]\n",
            "loss: 0.001052  [51200/60000]\n",
            "loss: 0.001137  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.093303 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.004541  [    0/60000]\n",
            "loss: 0.001543  [ 6400/60000]\n",
            "loss: 0.001627  [12800/60000]\n",
            "loss: 0.001256  [19200/60000]\n",
            "loss: 0.001087  [25600/60000]\n",
            "loss: 0.002345  [32000/60000]\n",
            "loss: 0.001114  [38400/60000]\n",
            "loss: 0.000744  [44800/60000]\n",
            "loss: 0.000301  [51200/60000]\n",
            "loss: 0.000700  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.8%, Avg loss: 0.136555 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.003614  [    0/60000]\n",
            "loss: 0.012439  [ 6400/60000]\n",
            "loss: 0.005040  [12800/60000]\n",
            "loss: 0.001089  [19200/60000]\n",
            "loss: 0.000987  [25600/60000]\n",
            "loss: 0.001063  [32000/60000]\n",
            "loss: 0.002578  [38400/60000]\n",
            "loss: 0.008229  [44800/60000]\n",
            "loss: 0.004208  [51200/60000]\n",
            "loss: 0.025103  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.8%, Avg loss: 0.126529 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.002700  [    0/60000]\n",
            "loss: 0.004517  [ 6400/60000]\n",
            "loss: 0.002109  [12800/60000]\n",
            "loss: 0.002291  [19200/60000]\n",
            "loss: 0.003109  [25600/60000]\n",
            "loss: 0.000398  [32000/60000]\n",
            "loss: 0.022335  [38400/60000]\n",
            "loss: 0.000159  [44800/60000]\n",
            "loss: 0.002694  [51200/60000]\n",
            "loss: 0.000209  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.095565 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.009894  [    0/60000]\n",
            "loss: 0.009913  [ 6400/60000]\n",
            "loss: 0.000921  [12800/60000]\n",
            "loss: 0.008762  [19200/60000]\n",
            "loss: 0.002974  [25600/60000]\n",
            "loss: 0.003971  [32000/60000]\n",
            "loss: 0.001028  [38400/60000]\n",
            "loss: 0.000357  [44800/60000]\n",
            "loss: 0.000392  [51200/60000]\n",
            "loss: 0.001470  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.084771 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.002772  [    0/60000]\n",
            "loss: 0.017049  [ 6400/60000]\n",
            "loss: 0.001512  [12800/60000]\n",
            "loss: 0.005050  [19200/60000]\n",
            "loss: 0.000974  [25600/60000]\n",
            "loss: 0.001848  [32000/60000]\n",
            "loss: 0.000480  [38400/60000]\n",
            "loss: 0.000192  [44800/60000]\n",
            "loss: 0.001377  [51200/60000]\n",
            "loss: 0.001586  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.087027 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.001983  [    0/60000]\n",
            "loss: 0.000881  [ 6400/60000]\n",
            "loss: 0.003532  [12800/60000]\n",
            "loss: 0.001465  [19200/60000]\n",
            "loss: 0.000403  [25600/60000]\n",
            "loss: 0.003365  [32000/60000]\n",
            "loss: 0.009776  [38400/60000]\n",
            "loss: 0.000095  [44800/60000]\n",
            "loss: 0.003987  [51200/60000]\n",
            "loss: 0.009769  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.084747 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.007421  [    0/60000]\n",
            "loss: 0.002314  [ 6400/60000]\n",
            "loss: 0.000917  [12800/60000]\n",
            "loss: 0.004303  [19200/60000]\n",
            "loss: 0.000395  [25600/60000]\n",
            "loss: 0.009812  [32000/60000]\n",
            "loss: 0.026922  [38400/60000]\n",
            "loss: 0.001366  [44800/60000]\n",
            "loss: 0.051624  [51200/60000]\n",
            "loss: 0.118262  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.124993 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.002945  [    0/60000]\n",
            "loss: 0.006448  [ 6400/60000]\n",
            "loss: 0.019209  [12800/60000]\n",
            "loss: 0.018256  [19200/60000]\n",
            "loss: 0.005112  [25600/60000]\n",
            "loss: 0.012105  [32000/60000]\n",
            "loss: 0.002989  [38400/60000]\n",
            "loss: 0.202784  [44800/60000]\n",
            "loss: 0.037187  [51200/60000]\n",
            "loss: 0.039653  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 94.5%, Avg loss: 0.301588 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.006289  [    0/60000]\n",
            "loss: 0.004488  [ 6400/60000]\n",
            "loss: 0.035719  [12800/60000]\n",
            "loss: 0.001825  [19200/60000]\n",
            "loss: 0.030839  [25600/60000]\n",
            "loss: 0.031619  [32000/60000]\n",
            "loss: 0.016396  [38400/60000]\n",
            "loss: 0.018518  [44800/60000]\n",
            "loss: 0.009077  [51200/60000]\n",
            "loss: 0.047818  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.0%, Avg loss: 0.220700 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.011978  [    0/60000]\n",
            "loss: 0.004155  [ 6400/60000]\n",
            "loss: 0.103838  [12800/60000]\n",
            "loss: 0.075996  [19200/60000]\n",
            "loss: 0.001441  [25600/60000]\n",
            "loss: 0.092032  [32000/60000]\n",
            "loss: 0.002795  [38400/60000]\n",
            "loss: 0.002731  [44800/60000]\n",
            "loss: 0.034045  [51200/60000]\n",
            "loss: 0.011599  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.7%, Avg loss: 0.197286 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.001768  [    0/60000]\n",
            "loss: 0.015998  [ 6400/60000]\n",
            "loss: 0.008736  [12800/60000]\n",
            "loss: 0.003516  [19200/60000]\n",
            "loss: 0.012341  [25600/60000]\n",
            "loss: 0.012469  [32000/60000]\n",
            "loss: 0.005174  [38400/60000]\n",
            "loss: 0.004572  [44800/60000]\n",
            "loss: 0.000233  [51200/60000]\n",
            "loss: 0.001530  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.8%, Avg loss: 0.136297 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.001477  [    0/60000]\n",
            "loss: 0.003801  [ 6400/60000]\n",
            "loss: 0.001013  [12800/60000]\n",
            "loss: 0.002058  [19200/60000]\n",
            "loss: 0.000736  [25600/60000]\n",
            "loss: 0.001529  [32000/60000]\n",
            "loss: 0.002362  [38400/60000]\n",
            "loss: 0.000753  [44800/60000]\n",
            "loss: 0.001307  [51200/60000]\n",
            "loss: 0.020599  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.9%, Avg loss: 0.130519 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.003550  [    0/60000]\n",
            "loss: 0.008126  [ 6400/60000]\n",
            "loss: 0.000481  [12800/60000]\n",
            "loss: 0.000935  [19200/60000]\n",
            "loss: 0.001747  [25600/60000]\n",
            "loss: 0.001745  [32000/60000]\n",
            "loss: 0.008444  [38400/60000]\n",
            "loss: 0.000375  [44800/60000]\n",
            "loss: 0.001329  [51200/60000]\n",
            "loss: 0.004107  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.103036 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.001575  [    0/60000]\n",
            "loss: 0.003044  [ 6400/60000]\n",
            "loss: 0.000648  [12800/60000]\n",
            "loss: 0.001329  [19200/60000]\n",
            "loss: 0.000259  [25600/60000]\n",
            "loss: 0.002231  [32000/60000]\n",
            "loss: 0.004224  [38400/60000]\n",
            "loss: 0.001042  [44800/60000]\n",
            "loss: 0.001229  [51200/60000]\n",
            "loss: 0.015022  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.102054 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.001515  [    0/60000]\n",
            "loss: 0.001267  [ 6400/60000]\n",
            "loss: 0.000688  [12800/60000]\n",
            "loss: 0.000505  [19200/60000]\n",
            "loss: 0.000656  [25600/60000]\n",
            "loss: 0.000908  [32000/60000]\n",
            "loss: 0.000178  [38400/60000]\n",
            "loss: 0.000319  [44800/60000]\n",
            "loss: 0.000436  [51200/60000]\n",
            "loss: 0.008843  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.122747 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.007725  [    0/60000]\n",
            "loss: 0.005233  [ 6400/60000]\n",
            "loss: 0.001026  [12800/60000]\n",
            "loss: 0.002497  [19200/60000]\n",
            "loss: 0.003136  [25600/60000]\n",
            "loss: 0.043384  [32000/60000]\n",
            "loss: 0.038194  [38400/60000]\n",
            "loss: 0.001716  [44800/60000]\n",
            "loss: 0.001545  [51200/60000]\n",
            "loss: 0.003482  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.094320 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.012190  [    0/60000]\n",
            "loss: 0.001031  [ 6400/60000]\n",
            "loss: 0.063371  [12800/60000]\n",
            "loss: 0.049175  [19200/60000]\n",
            "loss: 0.003291  [25600/60000]\n",
            "loss: 0.080921  [32000/60000]\n",
            "loss: 0.000816  [38400/60000]\n",
            "loss: 0.000275  [44800/60000]\n",
            "loss: 0.003023  [51200/60000]\n",
            "loss: 0.011794  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.9%, Avg loss: 0.130083 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.004186  [    0/60000]\n",
            "loss: 0.006205  [ 6400/60000]\n",
            "loss: 0.000476  [12800/60000]\n",
            "loss: 0.022989  [19200/60000]\n",
            "loss: 0.002886  [25600/60000]\n",
            "loss: 0.000580  [32000/60000]\n",
            "loss: 0.003117  [38400/60000]\n",
            "loss: 0.002871  [44800/60000]\n",
            "loss: 0.000284  [51200/60000]\n",
            "loss: 0.001628  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 0.096570 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.000894  [    0/60000]\n",
            "loss: 0.011713  [ 6400/60000]\n",
            "loss: 0.000957  [12800/60000]\n",
            "loss: 0.001029  [19200/60000]\n",
            "loss: 0.006643  [25600/60000]\n",
            "loss: 0.000258  [32000/60000]\n",
            "loss: 0.032170  [38400/60000]\n",
            "loss: 0.018599  [44800/60000]\n",
            "loss: 0.002557  [51200/60000]\n",
            "loss: 0.025806  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.9%, Avg loss: 0.123670 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.003083  [    0/60000]\n",
            "loss: 0.002090  [ 6400/60000]\n",
            "loss: 0.000849  [12800/60000]\n",
            "loss: 0.000856  [19200/60000]\n",
            "loss: 0.006739  [25600/60000]\n",
            "loss: 0.000094  [32000/60000]\n",
            "loss: 0.000149  [38400/60000]\n",
            "loss: 0.001425  [44800/60000]\n",
            "loss: 0.013379  [51200/60000]\n",
            "loss: 0.106024  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.112996 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.001012  [    0/60000]\n",
            "loss: 0.005399  [ 6400/60000]\n",
            "loss: 0.002257  [12800/60000]\n",
            "loss: 0.002032  [19200/60000]\n",
            "loss: 0.004318  [25600/60000]\n",
            "loss: 0.004154  [32000/60000]\n",
            "loss: 0.004205  [38400/60000]\n",
            "loss: 0.075248  [44800/60000]\n",
            "loss: 0.006509  [51200/60000]\n",
            "loss: 0.000891  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.088716 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.003090  [    0/60000]\n",
            "loss: 0.002440  [ 6400/60000]\n",
            "loss: 0.000283  [12800/60000]\n",
            "loss: 0.001510  [19200/60000]\n",
            "loss: 0.000959  [25600/60000]\n",
            "loss: 0.000486  [32000/60000]\n",
            "loss: 0.001046  [38400/60000]\n",
            "loss: 0.000904  [44800/60000]\n",
            "loss: 0.002054  [51200/60000]\n",
            "loss: 0.001813  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.084994 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.004084  [    0/60000]\n",
            "loss: 0.002093  [ 6400/60000]\n",
            "loss: 0.000544  [12800/60000]\n",
            "loss: 0.002080  [19200/60000]\n",
            "loss: 0.002509  [25600/60000]\n",
            "loss: 0.000332  [32000/60000]\n",
            "loss: 0.001574  [38400/60000]\n",
            "loss: 0.000302  [44800/60000]\n",
            "loss: 0.000451  [51200/60000]\n",
            "loss: 0.002561  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.0%, Avg loss: 0.084134 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Train and test the model\n",
        "loss_fn, optimizer = sgd_optimizer(model, lr=0.1)\n",
        "epochs = 100\n",
        "losses = []\n",
        "accuracies = []\n",
        "mean_losses = []\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "    \n",
        "    loss = get_loss(test_dataloader, model, loss_fn)\n",
        "    acc = get_score(test_dataloader, model, loss_fn)\n",
        "    mean_loss = get_mean_loss(test_dataloader, model, loss_fn, optimizer)\n",
        "    \n",
        "    losses.append(loss)\n",
        "    accuracies.append(acc)\n",
        "    mean_losses.append(mean_loss)\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot the loss and accuracy\n",
        "for i in range(len(losses)):\n",
        "    plt.plot(i, losses[i], 'bo')\n",
        "    plt.plot(i, accuracies[i], 'ro')\n",
        "    plt.plot(i, mean_losses[i], 'go')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss/Accuracy/Mean Loss')\n",
        "    plt.legend(['Loss', 'Accuracy', 'Mean Loss'])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and test the model\n",
        "loss_fn, optimizer = sgd_optimizer(model, lr=0.01)\n",
        "epochs = 100\n",
        "losses = []\n",
        "accuracies = []\n",
        "mean_losses = []\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "    \n",
        "    loss = get_loss(test_dataloader, model, loss_fn)\n",
        "    acc = get_score(test_dataloader, model, loss_fn)\n",
        "    mean_loss = get_mean_loss(test_dataloader, model, loss_fn, optimizer)\n",
        "    \n",
        "    losses.append(loss)\n",
        "    accuracies.append(acc)\n",
        "    mean_losses.append(mean_loss)\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the loss and accuracy\n",
        "for i in range(len(losses)):\n",
        "    plt.plot(i, losses[i], 'bo')\n",
        "    plt.plot(i, accuracies[i], 'ro')\n",
        "    plt.plot(i, mean_losses[i], 'go')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss/Accuracy/Mean Loss')\n",
        "    plt.legend(['Loss', 'Accuracy', 'Mean Loss'])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and test the model\n",
        "loss_fn, optimizer = sgd_optimizer(model, lr=0.001)\n",
        "epochs = 100\n",
        "losses = []\n",
        "accuracies = []\n",
        "mean_losses = []\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "    \n",
        "    loss = get_loss(test_dataloader, model, loss_fn)\n",
        "    acc = get_score(test_dataloader, model, loss_fn)\n",
        "    mean_loss = get_mean_loss(test_dataloader, model, loss_fn, optimizer)\n",
        "    \n",
        "    losses.append(loss)\n",
        "    accuracies.append(acc)\n",
        "    mean_losses.append(mean_loss)\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the loss and accuracy\n",
        "for i in range(len(losses)):\n",
        "    plt.plot(i, losses[i], 'bo')\n",
        "    plt.plot(i, accuracies[i], 'ro')\n",
        "    plt.plot(i, mean_losses[i], 'go')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss/Accuracy/Mean Loss')\n",
        "    plt.legend(['Loss', 'Accuracy', 'Mean Loss'])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S16_96RS3O1H"
      },
      "source": [
        "# Question 2: Proposal for Practical Applications (40%)\n",
        "Look for a typical computer vision problem, such as:\n",
        "a. removing noise on the image\n",
        "\n",
        "b. increasing the resolution of the image\n",
        "\n",
        "c. identifying objects in the image\n",
        "\n",
        "d. segmenting the area to which the image belongs\n",
        "\n",
        "e. estimating the depth of an object\n",
        "\n",
        "f. estimating the motion of two object in different frames\n",
        "\n",
        "h. others\n",
        "\n",
        "Discuss possible applications of this problem in life, e.g. image editing systems in your phone, improved quality of the old film, sweeping robot avoiding obstacles, unlocks the face of the mobile phone, identifies the cancer area according to the medical scan image, determines the identity according to the face, identifies the trash can on the road, and the detection system tracks the target object, etc.\n",
        "\n",
        "In this question, you need to do\n",
        "1. Clearly define the problem and describe its application scenarios\n",
        "2. Briefly describe a feasible solution based on image processing and traditional machine learning algorithms.\n",
        "3. Briefly describe a feasible deep learning-based solution.\n",
        "4. Compare the advantages and disadvantages of the two options.\n",
        "\n",
        "Hint1: Submit an individua report for question 2.\n",
        "\n",
        "Hint2: Well orginaze your report.\n",
        "\n",
        "Hint3: You can draw flow chart or inculde other figures for better understanding of your solution.  \n",
        "\n",
        "Please restrict your report within 800 words. In this question, you do not need to implement your solution. You only need to write down a proposal. Please submit this report in a seperate pdf. \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "a3.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMOJD0_jdzzg"
      },
      "source": [
        "## Computer vision 2022 Assignment 3: Deep Learning for Perception Tasks\n",
        "\n",
        "This assignment contains 2 questions. The first question gives you a basic understanding of the classifier. The second question requires you to write a simple proposal.\n",
        "\n",
        "# Question 1: A simple classifier (60%)\n",
        "\n",
        "For this exercise, we will provide a demo code showing how to train a network on a small dataset called FashionMinst. Please go through the following tutorials first. You will get a basic understanding about how to train an image classification network in pytorch. You can change the training scheme and the network structure. Please answer the following questions then. You can orginaze your own text and code cell to show the answer of each questions.\n",
        "\n",
        "\n",
        "Note: Please plot the loss curve for each experiment (2 point).\n",
        "\n",
        "\n",
        "Requirement:\n",
        "\n",
        "Q1.1 (1 point) Change the learning rate and train for 10 epochs. Fill this table:\n",
        "\n",
        "|Lr|Accuracy|\n",
        "|---|---|\n",
        "|1   |   19.92%   |\n",
        "|0.1|     87.22%     |\n",
        "|0.01|     83.67%    |\n",
        "|0.001  |    87.5%    |\n",
        "\n",
        "\n",
        "Q1.2 (2 point) Report the number of epochs when the accuracy reaches 90%. Fill this table:\n",
        "\n",
        "|Lr|Accuracy|Epoch|\n",
        "|---|---|---|\n",
        "|1   |   10%   |   11  |\n",
        "|0.1|     90%     |  174  |\n",
        "|0.01|    89.04%     |  273  |\n",
        "|0.001  |    87.2%    |   297  |\n",
        "\n",
        "\n",
        "Q1.3 (2 points) Compare the results in table 1 and table 2, what is your observation and your understanding of learning rate?\n",
        "\n",
        "From the table 1 and table 2, I notice that smaller learning rates necessitate more training epochs because of the fewer changes. On the other hand, larger learning rates result in faster changes.\n",
        "\n",
        "Q1.4 (3 point) Build a deeper/ wider network. Report the accuracy and the parameters for each structure. Parameters represent the number of trainable parameters in your model, e.g. a 3 x 3 conv has 9 parameters.\n",
        "\n",
        "|Structures|Accuracy|Parameters|\n",
        "|---|---|---|\n",
        "|Base   |   87.22%   |  669,706|\n",
        "|Deeper|  89.4%        |   674,836|\n",
        "|Wider|    90.3%     |   1,863,690|\n",
        "\n",
        "\n",
        "Q1.5 (2 points) Choose to do one of the following two tasks:\n",
        "\n",
        "a. Write a code to calculate the parameter and expian the code.\n",
        "\n",
        "OR\n",
        "\n",
        "b. Write done the process of how to calculate the parameters by hand. \n",
        "\n",
        "\n",
        "Q1.6 (1 points) What are your observations and conclusions for changing network structure?\n",
        "\n",
        "With the increasing of the parameters, the accuracy will also increase.\n",
        "\n",
        "Q1.7 (2 points) Calculate the mean of the gradients of the loss to all trainable parameters. Plot the gradients curve for the first 100 training steps. What are your observations? Note that this gradients will be saved with the training weight automatically after you call loss.backwards(). Hint: the mean of the gradients should be decreased.\n",
        "\n",
        "For more exlanation of q1.7, you could refer to the following simple instructions: https://colab.research.google.com/drive/1XAsyNegGSvMf3_B6MrsXht7-fHqtJ7OW?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-sqkIpLjpVsh"
      },
      "outputs": [],
      "source": [
        "import numpy as np # This is for mathematical operations\n",
        "\n",
        "# this is used in plotting \n",
        "import matplotlib.pyplot as plt \n",
        "import time\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "\n",
        "from a3 import *\n",
        "from torchinfo import summary\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%reload_ext autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:11dpocos) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdc16e7099eb41808db02b3b77b178e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Avg loss</td><td>▂▁▁▁▂▃▂▃▃▄▄▅▅▆▆▅▅▆▆▆▆▇▆▇▇▇▇▇▇███████████</td></tr><tr><td>accuracy</td><td>▁▄▅▅▅▅▅▆▆▆▆▅▆▅▆▆▇▆▇▇▇▅▇█████████████████</td></tr><tr><td>lr</td><td>▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Avg loss</td><td>0.94007</td></tr><tr><td>accuracy</td><td>0.9029</td></tr><tr><td>lr</td><td>0.1</td></tr><tr><td>optimizer</td><td>SGD</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">glorious-dew-25</strong>: <a href=\"https://wandb.ai/xiaolinzzz/Assignment%203/runs/11dpocos\" target=\"_blank\">https://wandb.ai/xiaolinzzz/Assignment%203/runs/11dpocos</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220517_195756-11dpocos/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:11dpocos). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/Users/malujie/Computer-vision/assignment 3/wandb/run-20220517_213728-238nqk4d</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/xiaolinzzz/Assignment%203/runs/238nqk4d\" target=\"_blank\">eager-music-26</a></strong> to <a href=\"https://wandb.ai/xiaolinzzz/Assignment%203\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/xiaolinzzz/Assignment%203/runs/238nqk4d?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f7cbde39880>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.init(project=\"Assignment 3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1wy3xhEx_x-1"
      },
      "outputs": [],
      "source": [
        "#### Tutorial Code\n",
        "####PyTorch has two primitives to work with data: torch.utils.data.DataLoader and torch.utils.data.Dataset. \n",
        "#####Dataset stores samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset.\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download training data from open datasets. \n",
        "##Every TorchVision Dataset includes two arguments: \n",
        "##transform and target_transform to modify the samples and labels respectively.\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNI4IusI_1ol"
      },
      "source": [
        "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset and supports automatic batching, sampling, shuffling, and multiprocess data loading. Here we define a batch size of 64, i.e. each element in the dataloader iterable will return a batch of 64 features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nQZ5l5Zs_4C3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
            "Shape of y:  torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "# wandb.log({'batch_size': batch_size})\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMtCU2LO_9Dk"
      },
      "source": [
        "To define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the init function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the GPU if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TRSp7pd3_6bS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "NeuralNetwork                            --                        --\n",
              "├─Flatten: 1-1                           [1, 784]                  --\n",
              "├─Sequential: 1-2                        [1, 10]                   --\n",
              "│    └─Linear: 2-1                       [1, 512]                  401,920\n",
              "│    └─ReLU: 2-2                         [1, 512]                  --\n",
              "│    └─Linear: 2-3                       [1, 512]                  262,656\n",
              "│    └─ReLU: 2-4                         [1, 512]                  --\n",
              "│    └─Linear: 2-5                       [1, 10]                   5,130\n",
              "==========================================================================================\n",
              "Total params: 669,706\n",
              "Trainable params: 669,706\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.67\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.01\n",
              "Params size (MB): 2.68\n",
              "Estimated Total Size (MB): 2.69\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "# Define model --> base\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "summary(model, (1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
        "    for item in params:\n",
        "        print(f'{item:>8}')\n",
        "    print(f'________\\n{sum(params):>8}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  401408\n",
            "     512\n",
            "  262144\n",
            "     512\n",
            "    5120\n",
            "      10\n",
            "________\n",
            "  669706\n"
          ]
        }
      ],
      "source": [
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "wider_model                              --                        --\n",
              "├─Flatten: 1-1                           [1, 784]                  --\n",
              "├─Sequential: 1-2                        [1, 10]                   --\n",
              "│    └─Linear: 2-1                       [1, 1024]                 803,840\n",
              "│    └─ReLU: 2-2                         [1, 1024]                 --\n",
              "│    └─Linear: 2-3                       [1, 1024]                 1,049,600\n",
              "│    └─ReLU: 2-4                         [1, 1024]                 --\n",
              "│    └─Linear: 2-5                       [1, 10]                   10,250\n",
              "==========================================================================================\n",
              "Total params: 1,863,690\n",
              "Trainable params: 1,863,690\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.86\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.02\n",
              "Params size (MB): 7.45\n",
              "Estimated Total Size (MB): 7.47\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a model --> wider\n",
        "# create a wider model\n",
        "class wider_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(wider_model, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "    \n",
        "model = wider_model().to(device)\n",
        "summary(model, (1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "deeper_model                             --\n",
              "├─Flatten: 1-1                           --\n",
              "├─Sequential: 1-2                        --\n",
              "│    └─Linear: 2-1                       401,920\n",
              "│    └─ReLU: 2-2                         --\n",
              "│    └─Linear: 2-3                       262,656\n",
              "│    └─ReLU: 2-4                         --\n",
              "│    └─Linear: 2-5                       5,130\n",
              "│    └─ReLU: 2-6                         --\n",
              "│    └─Linear: 2-7                       110\n",
              "=================================================================\n",
              "Total params: 669,816\n",
              "Trainable params: 669,816\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define a model --> deeper\n",
        "# create a deeper model\n",
        "class deeper_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(deeper_model, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "    \n",
        "model = deeper_model().to(device)\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  401408\n",
            "     512\n",
            "  262144\n",
            "     512\n",
            "    5120\n",
            "      10\n",
            "     100\n",
            "      10\n",
            "________\n",
            "  669816\n"
          ]
        }
      ],
      "source": [
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_fn, optimizer = sgd_optimizer(model, lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFZYEHY7ADvS"
      },
      "source": [
        "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "mJLACDm9AKxv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.326792  [    0/60000]\n",
            "loss: 1.269208  [ 6400/60000]\n",
            "loss: 0.815717  [12800/60000]\n",
            "loss: 0.801107  [19200/60000]\n",
            "loss: 0.655649  [25600/60000]\n",
            "loss: 0.513486  [32000/60000]\n",
            "loss: 0.539377  [38400/60000]\n",
            "loss: 0.604948  [44800/60000]\n",
            "loss: 0.630222  [51200/60000]\n",
            "loss: 0.540561  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.527581 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.412948  [    0/60000]\n",
            "loss: 0.448796  [ 6400/60000]\n",
            "loss: 0.388376  [12800/60000]\n",
            "loss: 0.455225  [19200/60000]\n",
            "loss: 0.441061  [25600/60000]\n",
            "loss: 0.476546  [32000/60000]\n",
            "loss: 0.419388  [38400/60000]\n",
            "loss: 0.520924  [44800/60000]\n",
            "loss: 0.486322  [51200/60000]\n",
            "loss: 0.454231  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.8%, Avg loss: 0.503730 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.355242  [    0/60000]\n",
            "loss: 0.363848  [ 6400/60000]\n",
            "loss: 0.347254  [12800/60000]\n",
            "loss: 0.346374  [19200/60000]\n",
            "loss: 0.355549  [25600/60000]\n",
            "loss: 0.422249  [32000/60000]\n",
            "loss: 0.354097  [38400/60000]\n",
            "loss: 0.475237  [44800/60000]\n",
            "loss: 0.422842  [51200/60000]\n",
            "loss: 0.432436  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.6%, Avg loss: 0.470096 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.292595  [    0/60000]\n",
            "loss: 0.321945  [ 6400/60000]\n",
            "loss: 0.301467  [12800/60000]\n",
            "loss: 0.306955  [19200/60000]\n",
            "loss: 0.338180  [25600/60000]\n",
            "loss: 0.383160  [32000/60000]\n",
            "loss: 0.317050  [38400/60000]\n",
            "loss: 0.433162  [44800/60000]\n",
            "loss: 0.385360  [51200/60000]\n",
            "loss: 0.398460  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.433600 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.228349  [    0/60000]\n",
            "loss: 0.303630  [ 6400/60000]\n",
            "loss: 0.270868  [12800/60000]\n",
            "loss: 0.285371  [19200/60000]\n",
            "loss: 0.333190  [25600/60000]\n",
            "loss: 0.373108  [32000/60000]\n",
            "loss: 0.289170  [38400/60000]\n",
            "loss: 0.412395  [44800/60000]\n",
            "loss: 0.357213  [51200/60000]\n",
            "loss: 0.366116  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.412321 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.209784  [    0/60000]\n",
            "loss: 0.286790  [ 6400/60000]\n",
            "loss: 0.233760  [12800/60000]\n",
            "loss: 0.257712  [19200/60000]\n",
            "loss: 0.316994  [25600/60000]\n",
            "loss: 0.342580  [32000/60000]\n",
            "loss: 0.259775  [38400/60000]\n",
            "loss: 0.398902  [44800/60000]\n",
            "loss: 0.353932  [51200/60000]\n",
            "loss: 0.346761  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.394587 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.208301  [    0/60000]\n",
            "loss: 0.275078  [ 6400/60000]\n",
            "loss: 0.210112  [12800/60000]\n",
            "loss: 0.236959  [19200/60000]\n",
            "loss: 0.316145  [25600/60000]\n",
            "loss: 0.323189  [32000/60000]\n",
            "loss: 0.267963  [38400/60000]\n",
            "loss: 0.372611  [44800/60000]\n",
            "loss: 0.321023  [51200/60000]\n",
            "loss: 0.325010  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.382832 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.167904  [    0/60000]\n",
            "loss: 0.269556  [ 6400/60000]\n",
            "loss: 0.209562  [12800/60000]\n",
            "loss: 0.229574  [19200/60000]\n",
            "loss: 0.330263  [25600/60000]\n",
            "loss: 0.311531  [32000/60000]\n",
            "loss: 0.246787  [38400/60000]\n",
            "loss: 0.358640  [44800/60000]\n",
            "loss: 0.324685  [51200/60000]\n",
            "loss: 0.327781  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.2%, Avg loss: 0.382991 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.163076  [    0/60000]\n",
            "loss: 0.252189  [ 6400/60000]\n",
            "loss: 0.206006  [12800/60000]\n",
            "loss: 0.223758  [19200/60000]\n",
            "loss: 0.313740  [25600/60000]\n",
            "loss: 0.307807  [32000/60000]\n",
            "loss: 0.239287  [38400/60000]\n",
            "loss: 0.347340  [44800/60000]\n",
            "loss: 0.311253  [51200/60000]\n",
            "loss: 0.342083  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.363691 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.142814  [    0/60000]\n",
            "loss: 0.246403  [ 6400/60000]\n",
            "loss: 0.201780  [12800/60000]\n",
            "loss: 0.219218  [19200/60000]\n",
            "loss: 0.287426  [25600/60000]\n",
            "loss: 0.275103  [32000/60000]\n",
            "loss: 0.211525  [38400/60000]\n",
            "loss: 0.313177  [44800/60000]\n",
            "loss: 0.296145  [51200/60000]\n",
            "loss: 0.316810  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.2%, Avg loss: 0.367395 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.131109  [    0/60000]\n",
            "loss: 0.225992  [ 6400/60000]\n",
            "loss: 0.198734  [12800/60000]\n",
            "loss: 0.217172  [19200/60000]\n",
            "loss: 0.290938  [25600/60000]\n",
            "loss: 0.265205  [32000/60000]\n",
            "loss: 0.213502  [38400/60000]\n",
            "loss: 0.308346  [44800/60000]\n",
            "loss: 0.268517  [51200/60000]\n",
            "loss: 0.277748  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.1%, Avg loss: 0.369649 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.123965  [    0/60000]\n",
            "loss: 0.202143  [ 6400/60000]\n",
            "loss: 0.180410  [12800/60000]\n",
            "loss: 0.206724  [19200/60000]\n",
            "loss: 0.280355  [25600/60000]\n",
            "loss: 0.271783  [32000/60000]\n",
            "loss: 0.192823  [38400/60000]\n",
            "loss: 0.280975  [44800/60000]\n",
            "loss: 0.258097  [51200/60000]\n",
            "loss: 0.264766  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.367059 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.126599  [    0/60000]\n",
            "loss: 0.195540  [ 6400/60000]\n",
            "loss: 0.182915  [12800/60000]\n",
            "loss: 0.200232  [19200/60000]\n",
            "loss: 0.295843  [25600/60000]\n",
            "loss: 0.270659  [32000/60000]\n",
            "loss: 0.194919  [38400/60000]\n",
            "loss: 0.278261  [44800/60000]\n",
            "loss: 0.273657  [51200/60000]\n",
            "loss: 0.266874  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.365997 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.122305  [    0/60000]\n",
            "loss: 0.160995  [ 6400/60000]\n",
            "loss: 0.204197  [12800/60000]\n",
            "loss: 0.214019  [19200/60000]\n",
            "loss: 0.276116  [25600/60000]\n",
            "loss: 0.267369  [32000/60000]\n",
            "loss: 0.176724  [38400/60000]\n",
            "loss: 0.242369  [44800/60000]\n",
            "loss: 0.262559  [51200/60000]\n",
            "loss: 0.260026  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.354458 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.112610  [    0/60000]\n",
            "loss: 0.175771  [ 6400/60000]\n",
            "loss: 0.169934  [12800/60000]\n",
            "loss: 0.180857  [19200/60000]\n",
            "loss: 0.268306  [25600/60000]\n",
            "loss: 0.244161  [32000/60000]\n",
            "loss: 0.201970  [38400/60000]\n",
            "loss: 0.253594  [44800/60000]\n",
            "loss: 0.235003  [51200/60000]\n",
            "loss: 0.241273  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.368819 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.123960  [    0/60000]\n",
            "loss: 0.128629  [ 6400/60000]\n",
            "loss: 0.149256  [12800/60000]\n",
            "loss: 0.171211  [19200/60000]\n",
            "loss: 0.245093  [25600/60000]\n",
            "loss: 0.260896  [32000/60000]\n",
            "loss: 0.151365  [38400/60000]\n",
            "loss: 0.219412  [44800/60000]\n",
            "loss: 0.268774  [51200/60000]\n",
            "loss: 0.216590  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.6%, Avg loss: 0.388727 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.123781  [    0/60000]\n",
            "loss: 0.144042  [ 6400/60000]\n",
            "loss: 0.164824  [12800/60000]\n",
            "loss: 0.140655  [19200/60000]\n",
            "loss: 0.273120  [25600/60000]\n",
            "loss: 0.266498  [32000/60000]\n",
            "loss: 0.161722  [38400/60000]\n",
            "loss: 0.220906  [44800/60000]\n",
            "loss: 0.334364  [51200/60000]\n",
            "loss: 0.232898  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.0%, Avg loss: 0.388041 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.097525  [    0/60000]\n",
            "loss: 0.136479  [ 6400/60000]\n",
            "loss: 0.174472  [12800/60000]\n",
            "loss: 0.146836  [19200/60000]\n",
            "loss: 0.224827  [25600/60000]\n",
            "loss: 0.247505  [32000/60000]\n",
            "loss: 0.200836  [38400/60000]\n",
            "loss: 0.237352  [44800/60000]\n",
            "loss: 0.233027  [51200/60000]\n",
            "loss: 0.217818  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.394840 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.108716  [    0/60000]\n",
            "loss: 0.128168  [ 6400/60000]\n",
            "loss: 0.162403  [12800/60000]\n",
            "loss: 0.139270  [19200/60000]\n",
            "loss: 0.231017  [25600/60000]\n",
            "loss: 0.223635  [32000/60000]\n",
            "loss: 0.130375  [38400/60000]\n",
            "loss: 0.205999  [44800/60000]\n",
            "loss: 0.260113  [51200/60000]\n",
            "loss: 0.210626  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.0%, Avg loss: 0.389989 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.107424  [    0/60000]\n",
            "loss: 0.113838  [ 6400/60000]\n",
            "loss: 0.135484  [12800/60000]\n",
            "loss: 0.123105  [19200/60000]\n",
            "loss: 0.200075  [25600/60000]\n",
            "loss: 0.235987  [32000/60000]\n",
            "loss: 0.143997  [38400/60000]\n",
            "loss: 0.209499  [44800/60000]\n",
            "loss: 0.215217  [51200/60000]\n",
            "loss: 0.240026  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.4%, Avg loss: 0.391303 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.102250  [    0/60000]\n",
            "loss: 0.091037  [ 6400/60000]\n",
            "loss: 0.139215  [12800/60000]\n",
            "loss: 0.125704  [19200/60000]\n",
            "loss: 0.298786  [25600/60000]\n",
            "loss: 0.173723  [32000/60000]\n",
            "loss: 0.173001  [38400/60000]\n",
            "loss: 0.159035  [44800/60000]\n",
            "loss: 0.183514  [51200/60000]\n",
            "loss: 0.186045  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.421017 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.109689  [    0/60000]\n",
            "loss: 0.087043  [ 6400/60000]\n",
            "loss: 0.115692  [12800/60000]\n",
            "loss: 0.087731  [19200/60000]\n",
            "loss: 0.201439  [25600/60000]\n",
            "loss: 0.176014  [32000/60000]\n",
            "loss: 0.144814  [38400/60000]\n",
            "loss: 0.178620  [44800/60000]\n",
            "loss: 0.237172  [51200/60000]\n",
            "loss: 0.186174  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.408306 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.099818  [    0/60000]\n",
            "loss: 0.114217  [ 6400/60000]\n",
            "loss: 0.138823  [12800/60000]\n",
            "loss: 0.140082  [19200/60000]\n",
            "loss: 0.154924  [25600/60000]\n",
            "loss: 0.168938  [32000/60000]\n",
            "loss: 0.144013  [38400/60000]\n",
            "loss: 0.148197  [44800/60000]\n",
            "loss: 0.204198  [51200/60000]\n",
            "loss: 0.227053  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.418628 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.089410  [    0/60000]\n",
            "loss: 0.081351  [ 6400/60000]\n",
            "loss: 0.150831  [12800/60000]\n",
            "loss: 0.118484  [19200/60000]\n",
            "loss: 0.216357  [25600/60000]\n",
            "loss: 0.163083  [32000/60000]\n",
            "loss: 0.099740  [38400/60000]\n",
            "loss: 0.130509  [44800/60000]\n",
            "loss: 0.174409  [51200/60000]\n",
            "loss: 0.173022  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.441038 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.087148  [    0/60000]\n",
            "loss: 0.104842  [ 6400/60000]\n",
            "loss: 0.118403  [12800/60000]\n",
            "loss: 0.100298  [19200/60000]\n",
            "loss: 0.139132  [25600/60000]\n",
            "loss: 0.151601  [32000/60000]\n",
            "loss: 0.082290  [38400/60000]\n",
            "loss: 0.116078  [44800/60000]\n",
            "loss: 0.240829  [51200/60000]\n",
            "loss: 0.184209  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.435461 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.107196  [    0/60000]\n",
            "loss: 0.070719  [ 6400/60000]\n",
            "loss: 0.093779  [12800/60000]\n",
            "loss: 0.095725  [19200/60000]\n",
            "loss: 0.162377  [25600/60000]\n",
            "loss: 0.165947  [32000/60000]\n",
            "loss: 3.581084  [38400/60000]\n",
            "loss: 0.156775  [44800/60000]\n",
            "loss: 0.186899  [51200/60000]\n",
            "loss: 0.203201  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.420082 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.087191  [    0/60000]\n",
            "loss: 0.065045  [ 6400/60000]\n",
            "loss: 0.100126  [12800/60000]\n",
            "loss: 0.109723  [19200/60000]\n",
            "loss: 0.217539  [25600/60000]\n",
            "loss: 0.177163  [32000/60000]\n",
            "loss: 0.120364  [38400/60000]\n",
            "loss: 0.148477  [44800/60000]\n",
            "loss: 0.180215  [51200/60000]\n",
            "loss: 0.160197  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.456481 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.079231  [    0/60000]\n",
            "loss: 0.053494  [ 6400/60000]\n",
            "loss: 0.109658  [12800/60000]\n",
            "loss: 0.081426  [19200/60000]\n",
            "loss: 0.137578  [25600/60000]\n",
            "loss: 0.169956  [32000/60000]\n",
            "loss: 0.136639  [38400/60000]\n",
            "loss: 0.141897  [44800/60000]\n",
            "loss: 0.168716  [51200/60000]\n",
            "loss: 0.140638  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.489602 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.101323  [    0/60000]\n",
            "loss: 0.077752  [ 6400/60000]\n",
            "loss: 0.097372  [12800/60000]\n",
            "loss: 0.051612  [19200/60000]\n",
            "loss: 0.230346  [25600/60000]\n",
            "loss: 0.150062  [32000/60000]\n",
            "loss: 0.071433  [38400/60000]\n",
            "loss: 0.099597  [44800/60000]\n",
            "loss: 0.186984  [51200/60000]\n",
            "loss: 0.095203  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.436055 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.079924  [    0/60000]\n",
            "loss: 0.100321  [ 6400/60000]\n",
            "loss: 0.092957  [12800/60000]\n",
            "loss: 0.076641  [19200/60000]\n",
            "loss: 0.131171  [25600/60000]\n",
            "loss: 0.118533  [32000/60000]\n",
            "loss: 0.110512  [38400/60000]\n",
            "loss: 0.121965  [44800/60000]\n",
            "loss: 0.152955  [51200/60000]\n",
            "loss: 0.147562  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.7%, Avg loss: 0.439738 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.087197  [    0/60000]\n",
            "loss: 0.074197  [ 6400/60000]\n",
            "loss: 0.107256  [12800/60000]\n",
            "loss: 0.078487  [19200/60000]\n",
            "loss: 0.102790  [25600/60000]\n",
            "loss: 0.140968  [32000/60000]\n",
            "loss: 0.083647  [38400/60000]\n",
            "loss: 0.133599  [44800/60000]\n",
            "loss: 0.183604  [51200/60000]\n",
            "loss: 0.266746  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.460295 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.087346  [    0/60000]\n",
            "loss: 0.084948  [ 6400/60000]\n",
            "loss: 0.069469  [12800/60000]\n",
            "loss: 0.141734  [19200/60000]\n",
            "loss: 0.146024  [25600/60000]\n",
            "loss: 0.131525  [32000/60000]\n",
            "loss: 0.079483  [38400/60000]\n",
            "loss: 0.090434  [44800/60000]\n",
            "loss: 0.075082  [51200/60000]\n",
            "loss: 0.089908  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.3%, Avg loss: 0.543814 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.177442  [    0/60000]\n",
            "loss: 0.128394  [ 6400/60000]\n",
            "loss: 0.074784  [12800/60000]\n",
            "loss: 0.054911  [19200/60000]\n",
            "loss: 0.153246  [25600/60000]\n",
            "loss: 0.187000  [32000/60000]\n",
            "loss: 0.080039  [38400/60000]\n",
            "loss: 0.085899  [44800/60000]\n",
            "loss: 0.123124  [51200/60000]\n",
            "loss: 0.138713  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.3%, Avg loss: 0.497306 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.065002  [    0/60000]\n",
            "loss: 0.092472  [ 6400/60000]\n",
            "loss: 0.042628  [12800/60000]\n",
            "loss: 0.040711  [19200/60000]\n",
            "loss: 0.117642  [25600/60000]\n",
            "loss: 0.158019  [32000/60000]\n",
            "loss: 0.085855  [38400/60000]\n",
            "loss: 0.091488  [44800/60000]\n",
            "loss: 0.105684  [51200/60000]\n",
            "loss: 0.081219  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.502938 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.093841  [    0/60000]\n",
            "loss: 0.063970  [ 6400/60000]\n",
            "loss: 0.082744  [12800/60000]\n",
            "loss: 0.074005  [19200/60000]\n",
            "loss: 0.166174  [25600/60000]\n",
            "loss: 0.104605  [32000/60000]\n",
            "loss: 0.066176  [38400/60000]\n",
            "loss: 0.114734  [44800/60000]\n",
            "loss: 0.120846  [51200/60000]\n",
            "loss: 0.171299  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.3%, Avg loss: 0.499034 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.081287  [    0/60000]\n",
            "loss: 0.066453  [ 6400/60000]\n",
            "loss: 0.083921  [12800/60000]\n",
            "loss: 0.059554  [19200/60000]\n",
            "loss: 0.097282  [25600/60000]\n",
            "loss: 0.145352  [32000/60000]\n",
            "loss: 0.053853  [38400/60000]\n",
            "loss: 0.146970  [44800/60000]\n",
            "loss: 0.115977  [51200/60000]\n",
            "loss: 0.240766  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.7%, Avg loss: 0.649006 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.089961  [    0/60000]\n",
            "loss: 0.046348  [ 6400/60000]\n",
            "loss: 0.068409  [12800/60000]\n",
            "loss: 0.029370  [19200/60000]\n",
            "loss: 0.110512  [25600/60000]\n",
            "loss: 0.128499  [32000/60000]\n",
            "loss: 0.130216  [38400/60000]\n",
            "loss: 0.076158  [44800/60000]\n",
            "loss: 0.148533  [51200/60000]\n",
            "loss: 0.189267  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.515493 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.077776  [    0/60000]\n",
            "loss: 0.036234  [ 6400/60000]\n",
            "loss: 0.069711  [12800/60000]\n",
            "loss: 0.140223  [19200/60000]\n",
            "loss: 0.127635  [25600/60000]\n",
            "loss: 0.090937  [32000/60000]\n",
            "loss: 0.052863  [38400/60000]\n",
            "loss: 0.061138  [44800/60000]\n",
            "loss: 0.109198  [51200/60000]\n",
            "loss: 0.049831  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.543060 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.063168  [    0/60000]\n",
            "loss: 0.131052  [ 6400/60000]\n",
            "loss: 0.077010  [12800/60000]\n",
            "loss: 0.062002  [19200/60000]\n",
            "loss: 0.086849  [25600/60000]\n",
            "loss: 0.140036  [32000/60000]\n",
            "loss: 0.061549  [38400/60000]\n",
            "loss: 0.067728  [44800/60000]\n",
            "loss: 0.096707  [51200/60000]\n",
            "loss: 0.268839  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.542662 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.110478  [    0/60000]\n",
            "loss: 0.092370  [ 6400/60000]\n",
            "loss: 0.049275  [12800/60000]\n",
            "loss: 0.024087  [19200/60000]\n",
            "loss: 0.048086  [25600/60000]\n",
            "loss: 0.098323  [32000/60000]\n",
            "loss: 0.063679  [38400/60000]\n",
            "loss: 0.141053  [44800/60000]\n",
            "loss: 0.098162  [51200/60000]\n",
            "loss: 0.187195  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.5%, Avg loss: 0.521767 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.062496  [    0/60000]\n",
            "loss: 0.054547  [ 6400/60000]\n",
            "loss: 0.043992  [12800/60000]\n",
            "loss: 0.021721  [19200/60000]\n",
            "loss: 0.066590  [25600/60000]\n",
            "loss: 0.099030  [32000/60000]\n",
            "loss: 0.094246  [38400/60000]\n",
            "loss: 0.092013  [44800/60000]\n",
            "loss: 0.143573  [51200/60000]\n",
            "loss: 0.058446  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.6%, Avg loss: 0.521740 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.145904  [    0/60000]\n",
            "loss: 0.051889  [ 6400/60000]\n",
            "loss: 0.052408  [12800/60000]\n",
            "loss: 0.067942  [19200/60000]\n",
            "loss: 0.090152  [25600/60000]\n",
            "loss: 0.091454  [32000/60000]\n",
            "loss: 0.084814  [38400/60000]\n",
            "loss: 0.059776  [44800/60000]\n",
            "loss: 0.114549  [51200/60000]\n",
            "loss: 0.118026  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.559847 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.067421  [    0/60000]\n",
            "loss: 0.078873  [ 6400/60000]\n",
            "loss: 0.070431  [12800/60000]\n",
            "loss: 0.034051  [19200/60000]\n",
            "loss: 0.087160  [25600/60000]\n",
            "loss: 0.099039  [32000/60000]\n",
            "loss: 0.074679  [38400/60000]\n",
            "loss: 0.069635  [44800/60000]\n",
            "loss: 0.073286  [51200/60000]\n",
            "loss: 0.087502  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.532712 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.052321  [    0/60000]\n",
            "loss: 0.089673  [ 6400/60000]\n",
            "loss: 0.021761  [12800/60000]\n",
            "loss: 0.032196  [19200/60000]\n",
            "loss: 0.072488  [25600/60000]\n",
            "loss: 0.113431  [32000/60000]\n",
            "loss: 0.055873  [38400/60000]\n",
            "loss: 0.042644  [44800/60000]\n",
            "loss: 0.103289  [51200/60000]\n",
            "loss: 0.114829  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.0%, Avg loss: 0.598189 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.137200  [    0/60000]\n",
            "loss: 0.057568  [ 6400/60000]\n",
            "loss: 0.040964  [12800/60000]\n",
            "loss: 0.030481  [19200/60000]\n",
            "loss: 0.158763  [25600/60000]\n",
            "loss: 0.118132  [32000/60000]\n",
            "loss: 0.082119  [38400/60000]\n",
            "loss: 0.062798  [44800/60000]\n",
            "loss: 0.143058  [51200/60000]\n",
            "loss: 0.097677  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.979021 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.205290  [    0/60000]\n",
            "loss: 0.051527  [ 6400/60000]\n",
            "loss: 0.039228  [12800/60000]\n",
            "loss: 0.040556  [19200/60000]\n",
            "loss: 0.142560  [25600/60000]\n",
            "loss: 0.115052  [32000/60000]\n",
            "loss: 0.040349  [38400/60000]\n",
            "loss: 0.038645  [44800/60000]\n",
            "loss: 0.098919  [51200/60000]\n",
            "loss: 0.091108  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.582718 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.091665  [    0/60000]\n",
            "loss: 0.118748  [ 6400/60000]\n",
            "loss: 0.049132  [12800/60000]\n",
            "loss: 0.053962  [19200/60000]\n",
            "loss: 0.069285  [25600/60000]\n",
            "loss: 0.072253  [32000/60000]\n",
            "loss: 0.090377  [38400/60000]\n",
            "loss: 0.043326  [44800/60000]\n",
            "loss: 0.102461  [51200/60000]\n",
            "loss: 0.076162  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.588120 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.196752  [    0/60000]\n",
            "loss: 0.050519  [ 6400/60000]\n",
            "loss: 0.026524  [12800/60000]\n",
            "loss: 0.022102  [19200/60000]\n",
            "loss: 0.028567  [25600/60000]\n",
            "loss: 0.050797  [32000/60000]\n",
            "loss: 0.057373  [38400/60000]\n",
            "loss: 0.039253  [44800/60000]\n",
            "loss: 0.082362  [51200/60000]\n",
            "loss: 0.082103  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.3%, Avg loss: 0.563215 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.092180  [    0/60000]\n",
            "loss: 0.020171  [ 6400/60000]\n",
            "loss: 0.080791  [12800/60000]\n",
            "loss: 0.026383  [19200/60000]\n",
            "loss: 0.059783  [25600/60000]\n",
            "loss: 0.174607  [32000/60000]\n",
            "loss: 0.084959  [38400/60000]\n",
            "loss: 0.124971  [44800/60000]\n",
            "loss: 0.112464  [51200/60000]\n",
            "loss: 0.084465  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.600487 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.073814  [    0/60000]\n",
            "loss: 0.056009  [ 6400/60000]\n",
            "loss: 0.055313  [12800/60000]\n",
            "loss: 0.063658  [19200/60000]\n",
            "loss: 0.099299  [25600/60000]\n",
            "loss: 0.093204  [32000/60000]\n",
            "loss: 0.063812  [38400/60000]\n",
            "loss: 0.051017  [44800/60000]\n",
            "loss: 0.121974  [51200/60000]\n",
            "loss: 0.076357  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.653511 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.121580  [    0/60000]\n",
            "loss: 0.023994  [ 6400/60000]\n",
            "loss: 0.009191  [12800/60000]\n",
            "loss: 0.031133  [19200/60000]\n",
            "loss: 0.193337  [25600/60000]\n",
            "loss: 0.062219  [32000/60000]\n",
            "loss: 0.057571  [38400/60000]\n",
            "loss: 0.018584  [44800/60000]\n",
            "loss: 0.045748  [51200/60000]\n",
            "loss: 0.150510  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.0%, Avg loss: 0.712692 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.070136  [    0/60000]\n",
            "loss: 0.013442  [ 6400/60000]\n",
            "loss: 0.133127  [12800/60000]\n",
            "loss: 0.034051  [19200/60000]\n",
            "loss: 0.060500  [25600/60000]\n",
            "loss: 0.047065  [32000/60000]\n",
            "loss: 0.085816  [38400/60000]\n",
            "loss: 0.103857  [44800/60000]\n",
            "loss: 0.024822  [51200/60000]\n",
            "loss: 0.080143  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.570807 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.046194  [    0/60000]\n",
            "loss: 0.038057  [ 6400/60000]\n",
            "loss: 0.082629  [12800/60000]\n",
            "loss: 0.016124  [19200/60000]\n",
            "loss: 0.042398  [25600/60000]\n",
            "loss: 0.088998  [32000/60000]\n",
            "loss: 0.032064  [38400/60000]\n",
            "loss: 0.023063  [44800/60000]\n",
            "loss: 0.079168  [51200/60000]\n",
            "loss: 0.085266  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.657130 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.077609  [    0/60000]\n",
            "loss: 0.029345  [ 6400/60000]\n",
            "loss: 0.092616  [12800/60000]\n",
            "loss: 0.037373  [19200/60000]\n",
            "loss: 0.097891  [25600/60000]\n",
            "loss: 0.086930  [32000/60000]\n",
            "loss: 0.147394  [38400/60000]\n",
            "loss: 0.161788  [44800/60000]\n",
            "loss: 0.041478  [51200/60000]\n",
            "loss: 0.065804  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.636272 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.088659  [    0/60000]\n",
            "loss: 0.012718  [ 6400/60000]\n",
            "loss: 0.079584  [12800/60000]\n",
            "loss: 0.030906  [19200/60000]\n",
            "loss: 0.028368  [25600/60000]\n",
            "loss: 0.087834  [32000/60000]\n",
            "loss: 0.082502  [38400/60000]\n",
            "loss: 0.023476  [44800/60000]\n",
            "loss: 0.106972  [51200/60000]\n",
            "loss: 0.074412  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.8%, Avg loss: 0.624658 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.038888  [    0/60000]\n",
            "loss: 0.023474  [ 6400/60000]\n",
            "loss: 0.031785  [12800/60000]\n",
            "loss: 0.058052  [19200/60000]\n",
            "loss: 0.080353  [25600/60000]\n",
            "loss: 0.026641  [32000/60000]\n",
            "loss: 0.058841  [38400/60000]\n",
            "loss: 0.015502  [44800/60000]\n",
            "loss: 0.035588  [51200/60000]\n",
            "loss: 0.060965  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.654104 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.056212  [    0/60000]\n",
            "loss: 0.018368  [ 6400/60000]\n",
            "loss: 0.047866  [12800/60000]\n",
            "loss: 0.046923  [19200/60000]\n",
            "loss: 0.054130  [25600/60000]\n",
            "loss: 0.033242  [32000/60000]\n",
            "loss: 0.031112  [38400/60000]\n",
            "loss: 0.079898  [44800/60000]\n",
            "loss: 0.033653  [51200/60000]\n",
            "loss: 0.076317  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.666094 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.162712  [    0/60000]\n",
            "loss: 0.029323  [ 6400/60000]\n",
            "loss: 0.076005  [12800/60000]\n",
            "loss: 0.020488  [19200/60000]\n",
            "loss: 0.079662  [25600/60000]\n",
            "loss: 0.089402  [32000/60000]\n",
            "loss: 0.063875  [38400/60000]\n",
            "loss: 0.088948  [44800/60000]\n",
            "loss: 0.142528  [51200/60000]\n",
            "loss: 0.049859  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.603272 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.063530  [    0/60000]\n",
            "loss: 0.027445  [ 6400/60000]\n",
            "loss: 0.048677  [12800/60000]\n",
            "loss: 0.014802  [19200/60000]\n",
            "loss: 0.076196  [25600/60000]\n",
            "loss: 0.052217  [32000/60000]\n",
            "loss: 0.028703  [38400/60000]\n",
            "loss: 0.079182  [44800/60000]\n",
            "loss: 0.050423  [51200/60000]\n",
            "loss: 0.116044  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.613924 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.055685  [    0/60000]\n",
            "loss: 0.008781  [ 6400/60000]\n",
            "loss: 0.015379  [12800/60000]\n",
            "loss: 0.043414  [19200/60000]\n",
            "loss: 0.085017  [25600/60000]\n",
            "loss: 0.042319  [32000/60000]\n",
            "loss: 0.052506  [38400/60000]\n",
            "loss: 0.005092  [44800/60000]\n",
            "loss: 0.074364  [51200/60000]\n",
            "loss: 0.048823  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.583051 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.058857  [    0/60000]\n",
            "loss: 0.090871  [ 6400/60000]\n",
            "loss: 0.062522  [12800/60000]\n",
            "loss: 0.027962  [19200/60000]\n",
            "loss: 0.046870  [25600/60000]\n",
            "loss: 0.131968  [32000/60000]\n",
            "loss: 0.060563  [38400/60000]\n",
            "loss: 0.057449  [44800/60000]\n",
            "loss: 0.074502  [51200/60000]\n",
            "loss: 0.065536  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.5%, Avg loss: 0.670921 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.135751  [    0/60000]\n",
            "loss: 0.012128  [ 6400/60000]\n",
            "loss: 0.021825  [12800/60000]\n",
            "loss: 0.021700  [19200/60000]\n",
            "loss: 0.040292  [25600/60000]\n",
            "loss: 0.098145  [32000/60000]\n",
            "loss: 0.032789  [38400/60000]\n",
            "loss: 0.013186  [44800/60000]\n",
            "loss: 0.018213  [51200/60000]\n",
            "loss: 0.094143  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.3%, Avg loss: 0.708236 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.049691  [    0/60000]\n",
            "loss: 0.062632  [ 6400/60000]\n",
            "loss: 0.107869  [12800/60000]\n",
            "loss: 0.012324  [19200/60000]\n",
            "loss: 0.052408  [25600/60000]\n",
            "loss: 0.027624  [32000/60000]\n",
            "loss: 0.051886  [38400/60000]\n",
            "loss: 0.007707  [44800/60000]\n",
            "loss: 0.041491  [51200/60000]\n",
            "loss: 0.019760  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.6%, Avg loss: 0.686864 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.089866  [    0/60000]\n",
            "loss: 0.019515  [ 6400/60000]\n",
            "loss: 0.015274  [12800/60000]\n",
            "loss: 0.012904  [19200/60000]\n",
            "loss: 0.036955  [25600/60000]\n",
            "loss: 0.020449  [32000/60000]\n",
            "loss: 0.003787  [38400/60000]\n",
            "loss: 0.078410  [44800/60000]\n",
            "loss: 0.036021  [51200/60000]\n",
            "loss: 0.018369  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.9%, Avg loss: 0.682992 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.037068  [    0/60000]\n",
            "loss: 0.036963  [ 6400/60000]\n",
            "loss: 0.039295  [12800/60000]\n",
            "loss: 0.035637  [19200/60000]\n",
            "loss: 0.090238  [25600/60000]\n",
            "loss: 0.111074  [32000/60000]\n",
            "loss: 0.026585  [38400/60000]\n",
            "loss: 0.019371  [44800/60000]\n",
            "loss: 0.050333  [51200/60000]\n",
            "loss: 0.043651  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.5%, Avg loss: 0.685527 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.059703  [    0/60000]\n",
            "loss: 0.059660  [ 6400/60000]\n",
            "loss: 0.015364  [12800/60000]\n",
            "loss: 0.003982  [19200/60000]\n",
            "loss: 0.013170  [25600/60000]\n",
            "loss: 0.033941  [32000/60000]\n",
            "loss: 0.043949  [38400/60000]\n",
            "loss: 0.030780  [44800/60000]\n",
            "loss: 0.138556  [51200/60000]\n",
            "loss: 0.141592  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.694980 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.075418  [    0/60000]\n",
            "loss: 0.021263  [ 6400/60000]\n",
            "loss: 0.023879  [12800/60000]\n",
            "loss: 0.027105  [19200/60000]\n",
            "loss: 0.034640  [25600/60000]\n",
            "loss: 0.039227  [32000/60000]\n",
            "loss: 0.035210  [38400/60000]\n",
            "loss: 0.028399  [44800/60000]\n",
            "loss: 0.058119  [51200/60000]\n",
            "loss: 0.015731  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.5%, Avg loss: 0.727234 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.019665  [    0/60000]\n",
            "loss: 0.031951  [ 6400/60000]\n",
            "loss: 0.032671  [12800/60000]\n",
            "loss: 0.034732  [19200/60000]\n",
            "loss: 0.052743  [25600/60000]\n",
            "loss: 0.100586  [32000/60000]\n",
            "loss: 0.017374  [38400/60000]\n",
            "loss: 0.006948  [44800/60000]\n",
            "loss: 0.074044  [51200/60000]\n",
            "loss: 0.021427  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.734626 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.028740  [    0/60000]\n",
            "loss: 0.008396  [ 6400/60000]\n",
            "loss: 0.063831  [12800/60000]\n",
            "loss: 0.016319  [19200/60000]\n",
            "loss: 0.036399  [25600/60000]\n",
            "loss: 0.046972  [32000/60000]\n",
            "loss: 0.006616  [38400/60000]\n",
            "loss: 0.111129  [44800/60000]\n",
            "loss: 0.013392  [51200/60000]\n",
            "loss: 0.041854  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.6%, Avg loss: 0.733967 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.104222  [    0/60000]\n",
            "loss: 0.039705  [ 6400/60000]\n",
            "loss: 0.046270  [12800/60000]\n",
            "loss: 0.016318  [19200/60000]\n",
            "loss: 0.049806  [25600/60000]\n",
            "loss: 0.037471  [32000/60000]\n",
            "loss: 0.062987  [38400/60000]\n",
            "loss: 0.007681  [44800/60000]\n",
            "loss: 0.016469  [51200/60000]\n",
            "loss: 0.025733  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.3%, Avg loss: 0.841465 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.079154  [    0/60000]\n",
            "loss: 0.007794  [ 6400/60000]\n",
            "loss: 0.010603  [12800/60000]\n",
            "loss: 0.029671  [19200/60000]\n",
            "loss: 0.020192  [25600/60000]\n",
            "loss: 0.024477  [32000/60000]\n",
            "loss: 0.049438  [38400/60000]\n",
            "loss: 0.075504  [44800/60000]\n",
            "loss: 0.017398  [51200/60000]\n",
            "loss: 0.040544  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.9%, Avg loss: 0.679120 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.018749  [    0/60000]\n",
            "loss: 0.089131  [ 6400/60000]\n",
            "loss: 0.040309  [12800/60000]\n",
            "loss: 0.011322  [19200/60000]\n",
            "loss: 0.135728  [25600/60000]\n",
            "loss: 0.031140  [32000/60000]\n",
            "loss: 0.015499  [38400/60000]\n",
            "loss: 0.029957  [44800/60000]\n",
            "loss: 0.032505  [51200/60000]\n",
            "loss: 0.050464  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.0%, Avg loss: 0.748371 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.209013  [    0/60000]\n",
            "loss: 0.126892  [ 6400/60000]\n",
            "loss: 0.081178  [12800/60000]\n",
            "loss: 0.001428  [19200/60000]\n",
            "loss: 0.056081  [25600/60000]\n",
            "loss: 0.030088  [32000/60000]\n",
            "loss: 0.055708  [38400/60000]\n",
            "loss: 0.069826  [44800/60000]\n",
            "loss: 0.015249  [51200/60000]\n",
            "loss: 0.034191  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.9%, Avg loss: 0.730054 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.081323  [    0/60000]\n",
            "loss: 0.042432  [ 6400/60000]\n",
            "loss: 0.023992  [12800/60000]\n",
            "loss: 0.031390  [19200/60000]\n",
            "loss: 0.157918  [25600/60000]\n",
            "loss: 0.055346  [32000/60000]\n",
            "loss: 0.027103  [38400/60000]\n",
            "loss: 0.037205  [44800/60000]\n",
            "loss: 0.013761  [51200/60000]\n",
            "loss: 0.035352  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.780436 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.015350  [    0/60000]\n",
            "loss: 0.043239  [ 6400/60000]\n",
            "loss: 0.019320  [12800/60000]\n",
            "loss: 0.010272  [19200/60000]\n",
            "loss: 0.040600  [25600/60000]\n",
            "loss: 0.053738  [32000/60000]\n",
            "loss: 0.010938  [38400/60000]\n",
            "loss: 0.007455  [44800/60000]\n",
            "loss: 0.005816  [51200/60000]\n",
            "loss: 0.024714  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.782386 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.072873  [    0/60000]\n",
            "loss: 0.003662  [ 6400/60000]\n",
            "loss: 0.081093  [12800/60000]\n",
            "loss: 0.014808  [19200/60000]\n",
            "loss: 0.029822  [25600/60000]\n",
            "loss: 0.017267  [32000/60000]\n",
            "loss: 0.051478  [38400/60000]\n",
            "loss: 0.037807  [44800/60000]\n",
            "loss: 0.049453  [51200/60000]\n",
            "loss: 0.034020  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.741070 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.029739  [    0/60000]\n",
            "loss: 0.065312  [ 6400/60000]\n",
            "loss: 0.028020  [12800/60000]\n",
            "loss: 0.018972  [19200/60000]\n",
            "loss: 0.085254  [25600/60000]\n",
            "loss: 0.071102  [32000/60000]\n",
            "loss: 0.061318  [38400/60000]\n",
            "loss: 0.007111  [44800/60000]\n",
            "loss: 0.022398  [51200/60000]\n",
            "loss: 0.032661  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.1%, Avg loss: 0.803030 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.025800  [    0/60000]\n",
            "loss: 0.031234  [ 6400/60000]\n",
            "loss: 0.032170  [12800/60000]\n",
            "loss: 0.029399  [19200/60000]\n",
            "loss: 0.137710  [25600/60000]\n",
            "loss: 0.022338  [32000/60000]\n",
            "loss: 0.021547  [38400/60000]\n",
            "loss: 0.025262  [44800/60000]\n",
            "loss: 0.018815  [51200/60000]\n",
            "loss: 0.024381  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 86.6%, Avg loss: 1.018621 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.243801  [    0/60000]\n",
            "loss: 0.052099  [ 6400/60000]\n",
            "loss: 0.039846  [12800/60000]\n",
            "loss: 0.023673  [19200/60000]\n",
            "loss: 0.006352  [25600/60000]\n",
            "loss: 0.043113  [32000/60000]\n",
            "loss: 0.053300  [38400/60000]\n",
            "loss: 0.080394  [44800/60000]\n",
            "loss: 0.008389  [51200/60000]\n",
            "loss: 0.037889  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.9%, Avg loss: 0.735191 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.081741  [    0/60000]\n",
            "loss: 0.028433  [ 6400/60000]\n",
            "loss: 0.005728  [12800/60000]\n",
            "loss: 0.003275  [19200/60000]\n",
            "loss: 0.024252  [25600/60000]\n",
            "loss: 0.030878  [32000/60000]\n",
            "loss: 0.104146  [38400/60000]\n",
            "loss: 0.004233  [44800/60000]\n",
            "loss: 0.029109  [51200/60000]\n",
            "loss: 0.092468  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.781296 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.010399  [    0/60000]\n",
            "loss: 0.012339  [ 6400/60000]\n",
            "loss: 0.107636  [12800/60000]\n",
            "loss: 0.003419  [19200/60000]\n",
            "loss: 0.048851  [25600/60000]\n",
            "loss: 0.037438  [32000/60000]\n",
            "loss: 0.006309  [38400/60000]\n",
            "loss: 0.004805  [44800/60000]\n",
            "loss: 0.073715  [51200/60000]\n",
            "loss: 0.022211  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.744714 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.040123  [    0/60000]\n",
            "loss: 0.016806  [ 6400/60000]\n",
            "loss: 0.024494  [12800/60000]\n",
            "loss: 0.029176  [19200/60000]\n",
            "loss: 0.029558  [25600/60000]\n",
            "loss: 0.186123  [32000/60000]\n",
            "loss: 0.028977  [38400/60000]\n",
            "loss: 0.010690  [44800/60000]\n",
            "loss: 0.010321  [51200/60000]\n",
            "loss: 0.041700  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.776299 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.021568  [    0/60000]\n",
            "loss: 0.024901  [ 6400/60000]\n",
            "loss: 0.066203  [12800/60000]\n",
            "loss: 0.059384  [19200/60000]\n",
            "loss: 0.059974  [25600/60000]\n",
            "loss: 0.174423  [32000/60000]\n",
            "loss: 0.061829  [38400/60000]\n",
            "loss: 0.015498  [44800/60000]\n",
            "loss: 0.018030  [51200/60000]\n",
            "loss: 0.046746  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.6%, Avg loss: 0.791265 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.064565  [    0/60000]\n",
            "loss: 0.032531  [ 6400/60000]\n",
            "loss: 0.080280  [12800/60000]\n",
            "loss: 0.016183  [19200/60000]\n",
            "loss: 0.064747  [25600/60000]\n",
            "loss: 0.020957  [32000/60000]\n",
            "loss: 0.044254  [38400/60000]\n",
            "loss: 0.099651  [44800/60000]\n",
            "loss: 0.125124  [51200/60000]\n",
            "loss: 0.008447  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.797444 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.040199  [    0/60000]\n",
            "loss: 0.024049  [ 6400/60000]\n",
            "loss: 0.125397  [12800/60000]\n",
            "loss: 0.028726  [19200/60000]\n",
            "loss: 0.013183  [25600/60000]\n",
            "loss: 0.011399  [32000/60000]\n",
            "loss: 0.004161  [38400/60000]\n",
            "loss: 0.054696  [44800/60000]\n",
            "loss: 0.035542  [51200/60000]\n",
            "loss: 0.031724  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.7%, Avg loss: 0.807430 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.057537  [    0/60000]\n",
            "loss: 0.041129  [ 6400/60000]\n",
            "loss: 0.006249  [12800/60000]\n",
            "loss: 0.006749  [19200/60000]\n",
            "loss: 0.016116  [25600/60000]\n",
            "loss: 0.007248  [32000/60000]\n",
            "loss: 0.001954  [38400/60000]\n",
            "loss: 0.002788  [44800/60000]\n",
            "loss: 0.003629  [51200/60000]\n",
            "loss: 0.045906  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.805418 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.027262  [    0/60000]\n",
            "loss: 0.020200  [ 6400/60000]\n",
            "loss: 0.033930  [12800/60000]\n",
            "loss: 0.027325  [19200/60000]\n",
            "loss: 0.123073  [25600/60000]\n",
            "loss: 0.091203  [32000/60000]\n",
            "loss: 0.021642  [38400/60000]\n",
            "loss: 0.027713  [44800/60000]\n",
            "loss: 0.019794  [51200/60000]\n",
            "loss: 0.010590  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.780160 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.011042  [    0/60000]\n",
            "loss: 0.040585  [ 6400/60000]\n",
            "loss: 0.020724  [12800/60000]\n",
            "loss: 0.005412  [19200/60000]\n",
            "loss: 0.078276  [25600/60000]\n",
            "loss: 0.029406  [32000/60000]\n",
            "loss: 0.014846  [38400/60000]\n",
            "loss: 0.007584  [44800/60000]\n",
            "loss: 0.005712  [51200/60000]\n",
            "loss: 0.013424  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.9%, Avg loss: 0.800053 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.007159  [    0/60000]\n",
            "loss: 0.012595  [ 6400/60000]\n",
            "loss: 0.002354  [12800/60000]\n",
            "loss: 0.014929  [19200/60000]\n",
            "loss: 0.020292  [25600/60000]\n",
            "loss: 0.020143  [32000/60000]\n",
            "loss: 0.011018  [38400/60000]\n",
            "loss: 0.003166  [44800/60000]\n",
            "loss: 0.076572  [51200/60000]\n",
            "loss: 0.048551  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.818697 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.086470  [    0/60000]\n",
            "loss: 0.045532  [ 6400/60000]\n",
            "loss: 0.044486  [12800/60000]\n",
            "loss: 0.015559  [19200/60000]\n",
            "loss: 0.039784  [25600/60000]\n",
            "loss: 0.017553  [32000/60000]\n",
            "loss: 0.011832  [38400/60000]\n",
            "loss: 0.003482  [44800/60000]\n",
            "loss: 0.033225  [51200/60000]\n",
            "loss: 0.025665  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.848268 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.061291  [    0/60000]\n",
            "loss: 0.081491  [ 6400/60000]\n",
            "loss: 0.008397  [12800/60000]\n",
            "loss: 0.004017  [19200/60000]\n",
            "loss: 0.023133  [25600/60000]\n",
            "loss: 0.035330  [32000/60000]\n",
            "loss: 0.019394  [38400/60000]\n",
            "loss: 0.006425  [44800/60000]\n",
            "loss: 0.009476  [51200/60000]\n",
            "loss: 0.065159  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.780911 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.010229  [    0/60000]\n",
            "loss: 0.021381  [ 6400/60000]\n",
            "loss: 0.008104  [12800/60000]\n",
            "loss: 0.025541  [19200/60000]\n",
            "loss: 0.061497  [25600/60000]\n",
            "loss: 0.119813  [32000/60000]\n",
            "loss: 0.026001  [38400/60000]\n",
            "loss: 0.046318  [44800/60000]\n",
            "loss: 0.037162  [51200/60000]\n",
            "loss: 0.034252  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.8%, Avg loss: 0.790333 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.031553  [    0/60000]\n",
            "loss: 0.108318  [ 6400/60000]\n",
            "loss: 0.092944  [12800/60000]\n",
            "loss: 0.038951  [19200/60000]\n",
            "loss: 0.056487  [25600/60000]\n",
            "loss: 0.006640  [32000/60000]\n",
            "loss: 0.094585  [38400/60000]\n",
            "loss: 0.050066  [44800/60000]\n",
            "loss: 0.003235  [51200/60000]\n",
            "loss: 0.009514  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.4%, Avg loss: 0.773800 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.024720  [    0/60000]\n",
            "loss: 0.010402  [ 6400/60000]\n",
            "loss: 0.048953  [12800/60000]\n",
            "loss: 0.024502  [19200/60000]\n",
            "loss: 0.020283  [25600/60000]\n",
            "loss: 0.004587  [32000/60000]\n",
            "loss: 0.018321  [38400/60000]\n",
            "loss: 0.019450  [44800/60000]\n",
            "loss: 0.068526  [51200/60000]\n",
            "loss: 0.009396  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.825535 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.074984  [    0/60000]\n",
            "loss: 0.014166  [ 6400/60000]\n",
            "loss: 0.008885  [12800/60000]\n",
            "loss: 0.005487  [19200/60000]\n",
            "loss: 0.018090  [25600/60000]\n",
            "loss: 0.040357  [32000/60000]\n",
            "loss: 0.011953  [38400/60000]\n",
            "loss: 0.054900  [44800/60000]\n",
            "loss: 0.041765  [51200/60000]\n",
            "loss: 0.055925  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.8%, Avg loss: 0.773201 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.025845  [    0/60000]\n",
            "loss: 0.022580  [ 6400/60000]\n",
            "loss: 0.021931  [12800/60000]\n",
            "loss: 0.001304  [19200/60000]\n",
            "loss: 0.007699  [25600/60000]\n",
            "loss: 0.010258  [32000/60000]\n",
            "loss: 0.030780  [38400/60000]\n",
            "loss: 0.011142  [44800/60000]\n",
            "loss: 0.004714  [51200/60000]\n",
            "loss: 0.009509  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.8%, Avg loss: 0.862922 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.016801  [    0/60000]\n",
            "loss: 0.046410  [ 6400/60000]\n",
            "loss: 0.001117  [12800/60000]\n",
            "loss: 0.086048  [19200/60000]\n",
            "loss: 0.041436  [25600/60000]\n",
            "loss: 0.146904  [32000/60000]\n",
            "loss: 0.042168  [38400/60000]\n",
            "loss: 0.005513  [44800/60000]\n",
            "loss: 0.011985  [51200/60000]\n",
            "loss: 0.077681  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.836475 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.056595  [    0/60000]\n",
            "loss: 0.006107  [ 6400/60000]\n",
            "loss: 0.030208  [12800/60000]\n",
            "loss: 0.002592  [19200/60000]\n",
            "loss: 0.018003  [25600/60000]\n",
            "loss: 0.012044  [32000/60000]\n",
            "loss: 0.070962  [38400/60000]\n",
            "loss: 0.005228  [44800/60000]\n",
            "loss: 0.058886  [51200/60000]\n",
            "loss: 0.107329  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.866139 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.182569  [    0/60000]\n",
            "loss: 0.008933  [ 6400/60000]\n",
            "loss: 0.003307  [12800/60000]\n",
            "loss: 0.001847  [19200/60000]\n",
            "loss: 0.024311  [25600/60000]\n",
            "loss: 0.016354  [32000/60000]\n",
            "loss: 0.002754  [38400/60000]\n",
            "loss: 0.003755  [44800/60000]\n",
            "loss: 0.021398  [51200/60000]\n",
            "loss: 0.021102  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.838113 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.052284  [    0/60000]\n",
            "loss: 0.004050  [ 6400/60000]\n",
            "loss: 0.022208  [12800/60000]\n",
            "loss: 0.001508  [19200/60000]\n",
            "loss: 0.134365  [25600/60000]\n",
            "loss: 0.068491  [32000/60000]\n",
            "loss: 0.007259  [38400/60000]\n",
            "loss: 0.050621  [44800/60000]\n",
            "loss: 0.002827  [51200/60000]\n",
            "loss: 0.043548  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.5%, Avg loss: 0.845008 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "loss: 0.008357  [    0/60000]\n",
            "loss: 0.033536  [ 6400/60000]\n",
            "loss: 0.010770  [12800/60000]\n",
            "loss: 0.011140  [19200/60000]\n",
            "loss: 0.042641  [25600/60000]\n",
            "loss: 0.083501  [32000/60000]\n",
            "loss: 0.013139  [38400/60000]\n",
            "loss: 0.004562  [44800/60000]\n",
            "loss: 0.042331  [51200/60000]\n",
            "loss: 0.010393  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.7%, Avg loss: 0.836400 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "loss: 0.064282  [    0/60000]\n",
            "loss: 0.026445  [ 6400/60000]\n",
            "loss: 0.001179  [12800/60000]\n",
            "loss: 0.029260  [19200/60000]\n",
            "loss: 0.041442  [25600/60000]\n",
            "loss: 0.011256  [32000/60000]\n",
            "loss: 0.003713  [38400/60000]\n",
            "loss: 0.001358  [44800/60000]\n",
            "loss: 0.067752  [51200/60000]\n",
            "loss: 0.011859  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.825729 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "loss: 0.036271  [    0/60000]\n",
            "loss: 0.012894  [ 6400/60000]\n",
            "loss: 0.007489  [12800/60000]\n",
            "loss: 0.062523  [19200/60000]\n",
            "loss: 0.016796  [25600/60000]\n",
            "loss: 0.061400  [32000/60000]\n",
            "loss: 0.013348  [38400/60000]\n",
            "loss: 0.005652  [44800/60000]\n",
            "loss: 0.017628  [51200/60000]\n",
            "loss: 0.018561  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 0.797856 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "loss: 0.010440  [    0/60000]\n",
            "loss: 0.097616  [ 6400/60000]\n",
            "loss: 0.010366  [12800/60000]\n",
            "loss: 0.028009  [19200/60000]\n",
            "loss: 0.020986  [25600/60000]\n",
            "loss: 0.004238  [32000/60000]\n",
            "loss: 0.077061  [38400/60000]\n",
            "loss: 0.002120  [44800/60000]\n",
            "loss: 0.035418  [51200/60000]\n",
            "loss: 0.010976  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.0%, Avg loss: 0.932193 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "loss: 0.196347  [    0/60000]\n",
            "loss: 0.056687  [ 6400/60000]\n",
            "loss: 0.001539  [12800/60000]\n",
            "loss: 0.000818  [19200/60000]\n",
            "loss: 0.031833  [25600/60000]\n",
            "loss: 0.012015  [32000/60000]\n",
            "loss: 0.050101  [38400/60000]\n",
            "loss: 0.020400  [44800/60000]\n",
            "loss: 0.001021  [51200/60000]\n",
            "loss: 0.021886  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.844860 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "loss: 0.008302  [    0/60000]\n",
            "loss: 0.010790  [ 6400/60000]\n",
            "loss: 0.007905  [12800/60000]\n",
            "loss: 0.006416  [19200/60000]\n",
            "loss: 0.020064  [25600/60000]\n",
            "loss: 0.050187  [32000/60000]\n",
            "loss: 0.008541  [38400/60000]\n",
            "loss: 0.043405  [44800/60000]\n",
            "loss: 0.003013  [51200/60000]\n",
            "loss: 0.015389  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.856057 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "loss: 0.055921  [    0/60000]\n",
            "loss: 0.012556  [ 6400/60000]\n",
            "loss: 0.019331  [12800/60000]\n",
            "loss: 0.070388  [19200/60000]\n",
            "loss: 0.014872  [25600/60000]\n",
            "loss: 0.017595  [32000/60000]\n",
            "loss: 0.015616  [38400/60000]\n",
            "loss: 0.000915  [44800/60000]\n",
            "loss: 0.003942  [51200/60000]\n",
            "loss: 0.052548  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.7%, Avg loss: 0.839940 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "loss: 0.125084  [    0/60000]\n",
            "loss: 0.010209  [ 6400/60000]\n",
            "loss: 0.024269  [12800/60000]\n",
            "loss: 0.003578  [19200/60000]\n",
            "loss: 0.027253  [25600/60000]\n",
            "loss: 0.161514  [32000/60000]\n",
            "loss: 0.006499  [38400/60000]\n",
            "loss: 0.016318  [44800/60000]\n",
            "loss: 0.044758  [51200/60000]\n",
            "loss: 0.014607  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.792904 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "loss: 0.036658  [    0/60000]\n",
            "loss: 0.003305  [ 6400/60000]\n",
            "loss: 0.008536  [12800/60000]\n",
            "loss: 0.010735  [19200/60000]\n",
            "loss: 0.290639  [25600/60000]\n",
            "loss: 0.066904  [32000/60000]\n",
            "loss: 0.007520  [38400/60000]\n",
            "loss: 0.006365  [44800/60000]\n",
            "loss: 0.015594  [51200/60000]\n",
            "loss: 0.028743  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.2%, Avg loss: 0.845586 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "loss: 0.005038  [    0/60000]\n",
            "loss: 0.004694  [ 6400/60000]\n",
            "loss: 0.015029  [12800/60000]\n",
            "loss: 0.003640  [19200/60000]\n",
            "loss: 0.014558  [25600/60000]\n",
            "loss: 0.005036  [32000/60000]\n",
            "loss: 0.024732  [38400/60000]\n",
            "loss: 0.003573  [44800/60000]\n",
            "loss: 0.015192  [51200/60000]\n",
            "loss: 0.040285  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.9%, Avg loss: 0.818902 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "loss: 0.027830  [    0/60000]\n",
            "loss: 0.006053  [ 6400/60000]\n",
            "loss: 0.000813  [12800/60000]\n",
            "loss: 0.003245  [19200/60000]\n",
            "loss: 0.033967  [25600/60000]\n",
            "loss: 0.006810  [32000/60000]\n",
            "loss: 0.012560  [38400/60000]\n",
            "loss: 0.000636  [44800/60000]\n",
            "loss: 0.005069  [51200/60000]\n",
            "loss: 0.042158  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.833360 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "loss: 0.014028  [    0/60000]\n",
            "loss: 0.001116  [ 6400/60000]\n",
            "loss: 0.006154  [12800/60000]\n",
            "loss: 0.003195  [19200/60000]\n",
            "loss: 0.004499  [25600/60000]\n",
            "loss: 0.012557  [32000/60000]\n",
            "loss: 0.008521  [38400/60000]\n",
            "loss: 0.002153  [44800/60000]\n",
            "loss: 0.066056  [51200/60000]\n",
            "loss: 0.008551  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.2%, Avg loss: 0.897419 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "loss: 0.003848  [    0/60000]\n",
            "loss: 0.006392  [ 6400/60000]\n",
            "loss: 0.023783  [12800/60000]\n",
            "loss: 0.008346  [19200/60000]\n",
            "loss: 0.029920  [25600/60000]\n",
            "loss: 0.009766  [32000/60000]\n",
            "loss: 0.005475  [38400/60000]\n",
            "loss: 0.000801  [44800/60000]\n",
            "loss: 0.007088  [51200/60000]\n",
            "loss: 0.117648  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.812418 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "loss: 0.025271  [    0/60000]\n",
            "loss: 0.022087  [ 6400/60000]\n",
            "loss: 0.002085  [12800/60000]\n",
            "loss: 0.002068  [19200/60000]\n",
            "loss: 0.017285  [25600/60000]\n",
            "loss: 0.003686  [32000/60000]\n",
            "loss: 0.061137  [38400/60000]\n",
            "loss: 0.001015  [44800/60000]\n",
            "loss: 0.022604  [51200/60000]\n",
            "loss: 0.026833  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.2%, Avg loss: 0.869546 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "loss: 0.011750  [    0/60000]\n",
            "loss: 0.038901  [ 6400/60000]\n",
            "loss: 0.042964  [12800/60000]\n",
            "loss: 0.031115  [19200/60000]\n",
            "loss: 0.007939  [25600/60000]\n",
            "loss: 0.034072  [32000/60000]\n",
            "loss: 0.012830  [38400/60000]\n",
            "loss: 0.001205  [44800/60000]\n",
            "loss: 0.018278  [51200/60000]\n",
            "loss: 0.007337  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.873490 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "loss: 0.006613  [    0/60000]\n",
            "loss: 0.009269  [ 6400/60000]\n",
            "loss: 0.002235  [12800/60000]\n",
            "loss: 0.031334  [19200/60000]\n",
            "loss: 0.004785  [25600/60000]\n",
            "loss: 0.004370  [32000/60000]\n",
            "loss: 0.002712  [38400/60000]\n",
            "loss: 0.002422  [44800/60000]\n",
            "loss: 0.003570  [51200/60000]\n",
            "loss: 0.011146  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 0.845083 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "loss: 0.013476  [    0/60000]\n",
            "loss: 0.015158  [ 6400/60000]\n",
            "loss: 0.001161  [12800/60000]\n",
            "loss: 0.000763  [19200/60000]\n",
            "loss: 0.029497  [25600/60000]\n",
            "loss: 0.001939  [32000/60000]\n",
            "loss: 0.000899  [38400/60000]\n",
            "loss: 0.002904  [44800/60000]\n",
            "loss: 0.025585  [51200/60000]\n",
            "loss: 0.009457  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.903403 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "loss: 0.001087  [    0/60000]\n",
            "loss: 0.005170  [ 6400/60000]\n",
            "loss: 0.019987  [12800/60000]\n",
            "loss: 0.040639  [19200/60000]\n",
            "loss: 0.036671  [25600/60000]\n",
            "loss: 0.017163  [32000/60000]\n",
            "loss: 0.005497  [38400/60000]\n",
            "loss: 0.080839  [44800/60000]\n",
            "loss: 0.044045  [51200/60000]\n",
            "loss: 0.032228  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.886829 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "loss: 0.122650  [    0/60000]\n",
            "loss: 0.011500  [ 6400/60000]\n",
            "loss: 0.004110  [12800/60000]\n",
            "loss: 0.005518  [19200/60000]\n",
            "loss: 0.009406  [25600/60000]\n",
            "loss: 0.007572  [32000/60000]\n",
            "loss: 0.019501  [38400/60000]\n",
            "loss: 0.003056  [44800/60000]\n",
            "loss: 0.051276  [51200/60000]\n",
            "loss: 0.022547  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.901145 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "loss: 0.003379  [    0/60000]\n",
            "loss: 0.026037  [ 6400/60000]\n",
            "loss: 0.007949  [12800/60000]\n",
            "loss: 0.000669  [19200/60000]\n",
            "loss: 0.010850  [25600/60000]\n",
            "loss: 0.010538  [32000/60000]\n",
            "loss: 0.018878  [38400/60000]\n",
            "loss: 0.015211  [44800/60000]\n",
            "loss: 0.042472  [51200/60000]\n",
            "loss: 0.003531  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.914652 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "loss: 0.033369  [    0/60000]\n",
            "loss: 0.021988  [ 6400/60000]\n",
            "loss: 0.001394  [12800/60000]\n",
            "loss: 0.046761  [19200/60000]\n",
            "loss: 0.004672  [25600/60000]\n",
            "loss: 0.088271  [32000/60000]\n",
            "loss: 0.016457  [38400/60000]\n",
            "loss: 0.011182  [44800/60000]\n",
            "loss: 0.008409  [51200/60000]\n",
            "loss: 0.025528  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.892700 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "loss: 0.006143  [    0/60000]\n",
            "loss: 0.001515  [ 6400/60000]\n",
            "loss: 0.003281  [12800/60000]\n",
            "loss: 0.003198  [19200/60000]\n",
            "loss: 0.014883  [25600/60000]\n",
            "loss: 0.011971  [32000/60000]\n",
            "loss: 0.010189  [38400/60000]\n",
            "loss: 0.027877  [44800/60000]\n",
            "loss: 0.003663  [51200/60000]\n",
            "loss: 0.016293  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.2%, Avg loss: 0.858474 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "loss: 0.030768  [    0/60000]\n",
            "loss: 0.011730  [ 6400/60000]\n",
            "loss: 0.014966  [12800/60000]\n",
            "loss: 0.001566  [19200/60000]\n",
            "loss: 0.011184  [25600/60000]\n",
            "loss: 0.003890  [32000/60000]\n",
            "loss: 0.020391  [38400/60000]\n",
            "loss: 0.007084  [44800/60000]\n",
            "loss: 0.002527  [51200/60000]\n",
            "loss: 0.011729  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.6%, Avg loss: 0.902238 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "loss: 0.043015  [    0/60000]\n",
            "loss: 0.001557  [ 6400/60000]\n",
            "loss: 0.006240  [12800/60000]\n",
            "loss: 0.010903  [19200/60000]\n",
            "loss: 0.011362  [25600/60000]\n",
            "loss: 0.001129  [32000/60000]\n",
            "loss: 0.003529  [38400/60000]\n",
            "loss: 0.001636  [44800/60000]\n",
            "loss: 0.006324  [51200/60000]\n",
            "loss: 0.004760  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.915754 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "loss: 0.001241  [    0/60000]\n",
            "loss: 0.008993  [ 6400/60000]\n",
            "loss: 0.015897  [12800/60000]\n",
            "loss: 0.005405  [19200/60000]\n",
            "loss: 0.013376  [25600/60000]\n",
            "loss: 0.087439  [32000/60000]\n",
            "loss: 0.030745  [38400/60000]\n",
            "loss: 0.004318  [44800/60000]\n",
            "loss: 0.000893  [51200/60000]\n",
            "loss: 0.039126  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 0.885671 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "loss: 0.008994  [    0/60000]\n",
            "loss: 0.006986  [ 6400/60000]\n",
            "loss: 0.001149  [12800/60000]\n",
            "loss: 0.001439  [19200/60000]\n",
            "loss: 0.014101  [25600/60000]\n",
            "loss: 0.025600  [32000/60000]\n",
            "loss: 0.002779  [38400/60000]\n",
            "loss: 0.000400  [44800/60000]\n",
            "loss: 0.033624  [51200/60000]\n",
            "loss: 0.044501  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 0.974988 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "loss: 0.039175  [    0/60000]\n",
            "loss: 0.086213  [ 6400/60000]\n",
            "loss: 0.008566  [12800/60000]\n",
            "loss: 0.071727  [19200/60000]\n",
            "loss: 0.037870  [25600/60000]\n",
            "loss: 0.014688  [32000/60000]\n",
            "loss: 0.017722  [38400/60000]\n",
            "loss: 0.010370  [44800/60000]\n",
            "loss: 0.038503  [51200/60000]\n",
            "loss: 0.050580  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.7%, Avg loss: 0.942173 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "loss: 0.032671  [    0/60000]\n",
            "loss: 0.007505  [ 6400/60000]\n",
            "loss: 0.012762  [12800/60000]\n",
            "loss: 0.009008  [19200/60000]\n",
            "loss: 0.108185  [25600/60000]\n",
            "loss: 0.025459  [32000/60000]\n",
            "loss: 0.053165  [38400/60000]\n",
            "loss: 0.029355  [44800/60000]\n",
            "loss: 0.066031  [51200/60000]\n",
            "loss: 0.007262  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.925417 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "loss: 0.031035  [    0/60000]\n",
            "loss: 0.006505  [ 6400/60000]\n",
            "loss: 0.007168  [12800/60000]\n",
            "loss: 0.010605  [19200/60000]\n",
            "loss: 0.005221  [25600/60000]\n",
            "loss: 0.008264  [32000/60000]\n",
            "loss: 0.001375  [38400/60000]\n",
            "loss: 0.003632  [44800/60000]\n",
            "loss: 0.029996  [51200/60000]\n",
            "loss: 0.054824  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.5%, Avg loss: 0.958919 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "loss: 0.002660  [    0/60000]\n",
            "loss: 0.001170  [ 6400/60000]\n",
            "loss: 0.008468  [12800/60000]\n",
            "loss: 0.014191  [19200/60000]\n",
            "loss: 0.019619  [25600/60000]\n",
            "loss: 0.039114  [32000/60000]\n",
            "loss: 0.009405  [38400/60000]\n",
            "loss: 0.008168  [44800/60000]\n",
            "loss: 0.006450  [51200/60000]\n",
            "loss: 0.016641  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.3%, Avg loss: 1.035276 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "loss: 0.005336  [    0/60000]\n",
            "loss: 0.030599  [ 6400/60000]\n",
            "loss: 0.000482  [12800/60000]\n",
            "loss: 0.003808  [19200/60000]\n",
            "loss: 0.213872  [25600/60000]\n",
            "loss: 0.006005  [32000/60000]\n",
            "loss: 0.015286  [38400/60000]\n",
            "loss: 0.001876  [44800/60000]\n",
            "loss: 0.003343  [51200/60000]\n",
            "loss: 0.146091  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.2%, Avg loss: 0.917541 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "loss: 0.007545  [    0/60000]\n",
            "loss: 0.013515  [ 6400/60000]\n",
            "loss: 0.057584  [12800/60000]\n",
            "loss: 0.040148  [19200/60000]\n",
            "loss: 0.003832  [25600/60000]\n",
            "loss: 0.020029  [32000/60000]\n",
            "loss: 0.004483  [38400/60000]\n",
            "loss: 0.001362  [44800/60000]\n",
            "loss: 0.006511  [51200/60000]\n",
            "loss: 0.089990  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.933590 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "loss: 0.045325  [    0/60000]\n",
            "loss: 0.001711  [ 6400/60000]\n",
            "loss: 0.017755  [12800/60000]\n",
            "loss: 0.005979  [19200/60000]\n",
            "loss: 0.116961  [25600/60000]\n",
            "loss: 0.009580  [32000/60000]\n",
            "loss: 0.071339  [38400/60000]\n",
            "loss: 0.002937  [44800/60000]\n",
            "loss: 0.003203  [51200/60000]\n",
            "loss: 0.006285  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.930217 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "loss: 0.009684  [    0/60000]\n",
            "loss: 0.004978  [ 6400/60000]\n",
            "loss: 0.001618  [12800/60000]\n",
            "loss: 0.007567  [19200/60000]\n",
            "loss: 0.001929  [25600/60000]\n",
            "loss: 0.000938  [32000/60000]\n",
            "loss: 0.009789  [38400/60000]\n",
            "loss: 0.013795  [44800/60000]\n",
            "loss: 0.000700  [51200/60000]\n",
            "loss: 0.007324  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.5%, Avg loss: 0.948613 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "loss: 0.008160  [    0/60000]\n",
            "loss: 0.004881  [ 6400/60000]\n",
            "loss: 0.003051  [12800/60000]\n",
            "loss: 0.002601  [19200/60000]\n",
            "loss: 0.007647  [25600/60000]\n",
            "loss: 0.084528  [32000/60000]\n",
            "loss: 0.023535  [38400/60000]\n",
            "loss: 0.138740  [44800/60000]\n",
            "loss: 0.006749  [51200/60000]\n",
            "loss: 0.013638  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.2%, Avg loss: 0.943548 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "loss: 0.006577  [    0/60000]\n",
            "loss: 0.049243  [ 6400/60000]\n",
            "loss: 0.006295  [12800/60000]\n",
            "loss: 0.001074  [19200/60000]\n",
            "loss: 0.004350  [25600/60000]\n",
            "loss: 0.008382  [32000/60000]\n",
            "loss: 0.012246  [38400/60000]\n",
            "loss: 0.040274  [44800/60000]\n",
            "loss: 0.067548  [51200/60000]\n",
            "loss: 0.010753  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.948239 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "loss: 0.011092  [    0/60000]\n",
            "loss: 0.000418  [ 6400/60000]\n",
            "loss: 0.013884  [12800/60000]\n",
            "loss: 0.004974  [19200/60000]\n",
            "loss: 0.011313  [25600/60000]\n",
            "loss: 0.003039  [32000/60000]\n",
            "loss: 0.008522  [38400/60000]\n",
            "loss: 0.016287  [44800/60000]\n",
            "loss: 0.020960  [51200/60000]\n",
            "loss: 0.027239  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.2%, Avg loss: 0.942990 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "loss: 0.006231  [    0/60000]\n",
            "loss: 0.005143  [ 6400/60000]\n",
            "loss: 0.053969  [12800/60000]\n",
            "loss: 0.003825  [19200/60000]\n",
            "loss: 0.003661  [25600/60000]\n",
            "loss: 0.009499  [32000/60000]\n",
            "loss: 0.000819  [38400/60000]\n",
            "loss: 0.005808  [44800/60000]\n",
            "loss: 0.000690  [51200/60000]\n",
            "loss: 0.014995  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.5%, Avg loss: 0.921744 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "loss: 0.057358  [    0/60000]\n",
            "loss: 0.017162  [ 6400/60000]\n",
            "loss: 0.002256  [12800/60000]\n",
            "loss: 0.005423  [19200/60000]\n",
            "loss: 0.067722  [25600/60000]\n",
            "loss: 0.023984  [32000/60000]\n",
            "loss: 0.003333  [38400/60000]\n",
            "loss: 0.000936  [44800/60000]\n",
            "loss: 0.023683  [51200/60000]\n",
            "loss: 0.003262  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.2%, Avg loss: 0.915445 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "loss: 0.010798  [    0/60000]\n",
            "loss: 0.000953  [ 6400/60000]\n",
            "loss: 0.032664  [12800/60000]\n",
            "loss: 0.013229  [19200/60000]\n",
            "loss: 0.010885  [25600/60000]\n",
            "loss: 0.031326  [32000/60000]\n",
            "loss: 0.002832  [38400/60000]\n",
            "loss: 0.000715  [44800/60000]\n",
            "loss: 0.001364  [51200/60000]\n",
            "loss: 0.018292  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.944289 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "loss: 0.049601  [    0/60000]\n",
            "loss: 0.002594  [ 6400/60000]\n",
            "loss: 0.000880  [12800/60000]\n",
            "loss: 0.023954  [19200/60000]\n",
            "loss: 0.026560  [25600/60000]\n",
            "loss: 0.002233  [32000/60000]\n",
            "loss: 0.002233  [38400/60000]\n",
            "loss: 0.005167  [44800/60000]\n",
            "loss: 0.008599  [51200/60000]\n",
            "loss: 0.056559  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.928739 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "loss: 0.052380  [    0/60000]\n",
            "loss: 0.000500  [ 6400/60000]\n",
            "loss: 0.002641  [12800/60000]\n",
            "loss: 0.001526  [19200/60000]\n",
            "loss: 0.003980  [25600/60000]\n",
            "loss: 0.058425  [32000/60000]\n",
            "loss: 0.011454  [38400/60000]\n",
            "loss: 0.000288  [44800/60000]\n",
            "loss: 0.003913  [51200/60000]\n",
            "loss: 0.003392  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.5%, Avg loss: 0.935139 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "loss: 0.002321  [    0/60000]\n",
            "loss: 0.017659  [ 6400/60000]\n",
            "loss: 0.094547  [12800/60000]\n",
            "loss: 0.000896  [19200/60000]\n",
            "loss: 0.004779  [25600/60000]\n",
            "loss: 0.020407  [32000/60000]\n",
            "loss: 0.019982  [38400/60000]\n",
            "loss: 0.001290  [44800/60000]\n",
            "loss: 0.011943  [51200/60000]\n",
            "loss: 0.045290  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.956107 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "loss: 0.028853  [    0/60000]\n",
            "loss: 0.001734  [ 6400/60000]\n",
            "loss: 0.019709  [12800/60000]\n",
            "loss: 0.012682  [19200/60000]\n",
            "loss: 0.000723  [25600/60000]\n",
            "loss: 0.116140  [32000/60000]\n",
            "loss: 0.003803  [38400/60000]\n",
            "loss: 0.009675  [44800/60000]\n",
            "loss: 0.007654  [51200/60000]\n",
            "loss: 0.009429  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.941288 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "loss: 0.023521  [    0/60000]\n",
            "loss: 0.000511  [ 6400/60000]\n",
            "loss: 0.000750  [12800/60000]\n",
            "loss: 0.006722  [19200/60000]\n",
            "loss: 0.011401  [25600/60000]\n",
            "loss: 0.000977  [32000/60000]\n",
            "loss: 0.001255  [38400/60000]\n",
            "loss: 0.000375  [44800/60000]\n",
            "loss: 0.005149  [51200/60000]\n",
            "loss: 0.111604  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 0.917955 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "loss: 0.009746  [    0/60000]\n",
            "loss: 0.001731  [ 6400/60000]\n",
            "loss: 0.043929  [12800/60000]\n",
            "loss: 0.003104  [19200/60000]\n",
            "loss: 0.019882  [25600/60000]\n",
            "loss: 0.001989  [32000/60000]\n",
            "loss: 0.056778  [38400/60000]\n",
            "loss: 0.021113  [44800/60000]\n",
            "loss: 0.000859  [51200/60000]\n",
            "loss: 0.005925  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.905022 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "loss: 0.002151  [    0/60000]\n",
            "loss: 0.009542  [ 6400/60000]\n",
            "loss: 0.000249  [12800/60000]\n",
            "loss: 0.003288  [19200/60000]\n",
            "loss: 0.005962  [25600/60000]\n",
            "loss: 0.033214  [32000/60000]\n",
            "loss: 0.000210  [38400/60000]\n",
            "loss: 0.003840  [44800/60000]\n",
            "loss: 0.011912  [51200/60000]\n",
            "loss: 0.001907  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.8%, Avg loss: 0.968850 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "loss: 0.021809  [    0/60000]\n",
            "loss: 0.001671  [ 6400/60000]\n",
            "loss: 0.003449  [12800/60000]\n",
            "loss: 0.003259  [19200/60000]\n",
            "loss: 0.001585  [25600/60000]\n",
            "loss: 0.005652  [32000/60000]\n",
            "loss: 0.000782  [38400/60000]\n",
            "loss: 0.000692  [44800/60000]\n",
            "loss: 0.008682  [51200/60000]\n",
            "loss: 0.048408  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 0.989377 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "loss: 0.000473  [    0/60000]\n",
            "loss: 0.001294  [ 6400/60000]\n",
            "loss: 0.000780  [12800/60000]\n",
            "loss: 0.001083  [19200/60000]\n",
            "loss: 0.003252  [25600/60000]\n",
            "loss: 0.018700  [32000/60000]\n",
            "loss: 0.005380  [38400/60000]\n",
            "loss: 0.000643  [44800/60000]\n",
            "loss: 0.031011  [51200/60000]\n",
            "loss: 0.000415  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.980698 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "loss: 0.000656  [    0/60000]\n",
            "loss: 0.006684  [ 6400/60000]\n",
            "loss: 0.000915  [12800/60000]\n",
            "loss: 0.054337  [19200/60000]\n",
            "loss: 0.001348  [25600/60000]\n",
            "loss: 0.008245  [32000/60000]\n",
            "loss: 0.001387  [38400/60000]\n",
            "loss: 0.004081  [44800/60000]\n",
            "loss: 0.009256  [51200/60000]\n",
            "loss: 0.077951  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.3%, Avg loss: 1.065076 \n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "loss: 0.047181  [    0/60000]\n",
            "loss: 0.002855  [ 6400/60000]\n",
            "loss: 0.003979  [12800/60000]\n",
            "loss: 0.002409  [19200/60000]\n",
            "loss: 0.011453  [25600/60000]\n",
            "loss: 0.000417  [32000/60000]\n",
            "loss: 0.002223  [38400/60000]\n",
            "loss: 0.022020  [44800/60000]\n",
            "loss: 0.008456  [51200/60000]\n",
            "loss: 0.048781  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.995985 \n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "loss: 0.155017  [    0/60000]\n",
            "loss: 0.118227  [ 6400/60000]\n",
            "loss: 0.003227  [12800/60000]\n",
            "loss: 0.003543  [19200/60000]\n",
            "loss: 0.009608  [25600/60000]\n",
            "loss: 0.064553  [32000/60000]\n",
            "loss: 0.002408  [38400/60000]\n",
            "loss: 0.015634  [44800/60000]\n",
            "loss: 0.002004  [51200/60000]\n",
            "loss: 0.009129  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.2%, Avg loss: 0.949612 \n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "loss: 0.004266  [    0/60000]\n",
            "loss: 0.010920  [ 6400/60000]\n",
            "loss: 0.089061  [12800/60000]\n",
            "loss: 0.007029  [19200/60000]\n",
            "loss: 0.050282  [25600/60000]\n",
            "loss: 0.009162  [32000/60000]\n",
            "loss: 0.001994  [38400/60000]\n",
            "loss: 0.006868  [44800/60000]\n",
            "loss: 0.000190  [51200/60000]\n",
            "loss: 0.006840  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.955209 \n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "loss: 0.038931  [    0/60000]\n",
            "loss: 0.044769  [ 6400/60000]\n",
            "loss: 0.047139  [12800/60000]\n",
            "loss: 0.011388  [19200/60000]\n",
            "loss: 0.051359  [25600/60000]\n",
            "loss: 0.009265  [32000/60000]\n",
            "loss: 0.048272  [38400/60000]\n",
            "loss: 0.000729  [44800/60000]\n",
            "loss: 0.000826  [51200/60000]\n",
            "loss: 0.032715  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.5%, Avg loss: 0.974751 \n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "loss: 0.000998  [    0/60000]\n",
            "loss: 0.009527  [ 6400/60000]\n",
            "loss: 0.032495  [12800/60000]\n",
            "loss: 0.006663  [19200/60000]\n",
            "loss: 0.018390  [25600/60000]\n",
            "loss: 0.011169  [32000/60000]\n",
            "loss: 0.000284  [38400/60000]\n",
            "loss: 0.001807  [44800/60000]\n",
            "loss: 0.004668  [51200/60000]\n",
            "loss: 0.016077  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.8%, Avg loss: 0.967657 \n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "loss: 0.003173  [    0/60000]\n",
            "loss: 0.002170  [ 6400/60000]\n",
            "loss: 0.006307  [12800/60000]\n",
            "loss: 0.001707  [19200/60000]\n",
            "loss: 0.000807  [25600/60000]\n",
            "loss: 0.003145  [32000/60000]\n",
            "loss: 0.000688  [38400/60000]\n",
            "loss: 0.000719  [44800/60000]\n",
            "loss: 0.005003  [51200/60000]\n",
            "loss: 0.007119  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.960996 \n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "loss: 0.003242  [    0/60000]\n",
            "loss: 0.001747  [ 6400/60000]\n",
            "loss: 0.014344  [12800/60000]\n",
            "loss: 0.043144  [19200/60000]\n",
            "loss: 0.027563  [25600/60000]\n",
            "loss: 0.000257  [32000/60000]\n",
            "loss: 0.006037  [38400/60000]\n",
            "loss: 0.000187  [44800/60000]\n",
            "loss: 0.001395  [51200/60000]\n",
            "loss: 0.011037  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 1.051107 \n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "loss: 0.002739  [    0/60000]\n",
            "loss: 0.001426  [ 6400/60000]\n",
            "loss: 0.002149  [12800/60000]\n",
            "loss: 0.011671  [19200/60000]\n",
            "loss: 0.002468  [25600/60000]\n",
            "loss: 0.018342  [32000/60000]\n",
            "loss: 0.032433  [38400/60000]\n",
            "loss: 0.008176  [44800/60000]\n",
            "loss: 0.000403  [51200/60000]\n",
            "loss: 0.002400  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.8%, Avg loss: 0.985051 \n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "loss: 0.006955  [    0/60000]\n",
            "loss: 0.004954  [ 6400/60000]\n",
            "loss: 0.025721  [12800/60000]\n",
            "loss: 0.014117  [19200/60000]\n",
            "loss: 0.001575  [25600/60000]\n",
            "loss: 0.004198  [32000/60000]\n",
            "loss: 0.067669  [38400/60000]\n",
            "loss: 0.004277  [44800/60000]\n",
            "loss: 0.029636  [51200/60000]\n",
            "loss: 0.189454  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 0.929157 \n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "loss: 0.032440  [    0/60000]\n",
            "loss: 0.004559  [ 6400/60000]\n",
            "loss: 0.001705  [12800/60000]\n",
            "loss: 0.001212  [19200/60000]\n",
            "loss: 0.003477  [25600/60000]\n",
            "loss: 0.133209  [32000/60000]\n",
            "loss: 0.003581  [38400/60000]\n",
            "loss: 0.004362  [44800/60000]\n",
            "loss: 0.002053  [51200/60000]\n",
            "loss: 0.004601  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.890822 \n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "loss: 0.006684  [    0/60000]\n",
            "loss: 0.001752  [ 6400/60000]\n",
            "loss: 0.002559  [12800/60000]\n",
            "loss: 0.000421  [19200/60000]\n",
            "loss: 0.010222  [25600/60000]\n",
            "loss: 0.001633  [32000/60000]\n",
            "loss: 0.055636  [38400/60000]\n",
            "loss: 0.006018  [44800/60000]\n",
            "loss: 0.000795  [51200/60000]\n",
            "loss: 0.008265  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.915696 \n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "loss: 0.003444  [    0/60000]\n",
            "loss: 0.004253  [ 6400/60000]\n",
            "loss: 0.000336  [12800/60000]\n",
            "loss: 0.000754  [19200/60000]\n",
            "loss: 0.004467  [25600/60000]\n",
            "loss: 0.010804  [32000/60000]\n",
            "loss: 0.007457  [38400/60000]\n",
            "loss: 0.001475  [44800/60000]\n",
            "loss: 0.007926  [51200/60000]\n",
            "loss: 0.008903  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.2%, Avg loss: 0.956499 \n",
            "\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "loss: 0.007675  [    0/60000]\n",
            "loss: 0.000556  [ 6400/60000]\n",
            "loss: 0.000378  [12800/60000]\n",
            "loss: 0.014449  [19200/60000]\n",
            "loss: 0.006774  [25600/60000]\n",
            "loss: 0.010826  [32000/60000]\n",
            "loss: 0.009687  [38400/60000]\n",
            "loss: 0.001345  [44800/60000]\n",
            "loss: 0.005993  [51200/60000]\n",
            "loss: 0.002194  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.964745 \n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "loss: 0.001638  [    0/60000]\n",
            "loss: 0.007313  [ 6400/60000]\n",
            "loss: 0.000553  [12800/60000]\n",
            "loss: 0.013327  [19200/60000]\n",
            "loss: 0.041918  [25600/60000]\n",
            "loss: 0.006978  [32000/60000]\n",
            "loss: 0.002069  [38400/60000]\n",
            "loss: 0.000834  [44800/60000]\n",
            "loss: 0.003337  [51200/60000]\n",
            "loss: 0.006287  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.2%, Avg loss: 0.914868 \n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "loss: 0.007165  [    0/60000]\n",
            "loss: 0.103251  [ 6400/60000]\n",
            "loss: 0.000590  [12800/60000]\n",
            "loss: 0.000466  [19200/60000]\n",
            "loss: 0.007910  [25600/60000]\n",
            "loss: 0.002495  [32000/60000]\n",
            "loss: 0.001690  [38400/60000]\n",
            "loss: 0.004512  [44800/60000]\n",
            "loss: 0.004300  [51200/60000]\n",
            "loss: 0.002517  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 0.954981 \n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "loss: 0.001209  [    0/60000]\n",
            "loss: 0.011310  [ 6400/60000]\n",
            "loss: 0.009499  [12800/60000]\n",
            "loss: 0.002324  [19200/60000]\n",
            "loss: 0.002934  [25600/60000]\n",
            "loss: 0.000857  [32000/60000]\n",
            "loss: 0.001112  [38400/60000]\n",
            "loss: 0.000173  [44800/60000]\n",
            "loss: 0.000191  [51200/60000]\n",
            "loss: 0.001010  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.2%, Avg loss: 0.987742 \n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "loss: 0.000448  [    0/60000]\n",
            "loss: 0.025512  [ 6400/60000]\n",
            "loss: 0.019332  [12800/60000]\n",
            "loss: 0.001397  [19200/60000]\n",
            "loss: 0.023491  [25600/60000]\n",
            "loss: 0.082848  [32000/60000]\n",
            "loss: 0.001207  [38400/60000]\n",
            "loss: 0.007511  [44800/60000]\n",
            "loss: 0.000464  [51200/60000]\n",
            "loss: 0.050646  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.5%, Avg loss: 0.921911 \n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "loss: 0.013189  [    0/60000]\n",
            "loss: 0.043144  [ 6400/60000]\n",
            "loss: 0.003918  [12800/60000]\n",
            "loss: 0.002165  [19200/60000]\n",
            "loss: 0.020767  [25600/60000]\n",
            "loss: 0.039529  [32000/60000]\n",
            "loss: 0.015986  [38400/60000]\n",
            "loss: 0.002838  [44800/60000]\n",
            "loss: 0.032984  [51200/60000]\n",
            "loss: 0.117513  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 1.029436 \n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "loss: 0.001390  [    0/60000]\n",
            "loss: 0.001498  [ 6400/60000]\n",
            "loss: 0.000855  [12800/60000]\n",
            "loss: 0.008934  [19200/60000]\n",
            "loss: 0.051256  [25600/60000]\n",
            "loss: 0.002553  [32000/60000]\n",
            "loss: 0.047822  [38400/60000]\n",
            "loss: 0.000965  [44800/60000]\n",
            "loss: 0.037278  [51200/60000]\n",
            "loss: 0.023821  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.996520 \n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "loss: 0.000216  [    0/60000]\n",
            "loss: 0.002193  [ 6400/60000]\n",
            "loss: 0.001864  [12800/60000]\n",
            "loss: 0.061565  [19200/60000]\n",
            "loss: 0.001739  [25600/60000]\n",
            "loss: 0.003820  [32000/60000]\n",
            "loss: 0.002177  [38400/60000]\n",
            "loss: 0.006707  [44800/60000]\n",
            "loss: 0.037015  [51200/60000]\n",
            "loss: 0.018730  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 0.962174 \n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "loss: 0.007762  [    0/60000]\n",
            "loss: 0.005650  [ 6400/60000]\n",
            "loss: 0.003062  [12800/60000]\n",
            "loss: 0.057837  [19200/60000]\n",
            "loss: 0.002021  [25600/60000]\n",
            "loss: 0.027137  [32000/60000]\n",
            "loss: 0.001909  [38400/60000]\n",
            "loss: 0.009160  [44800/60000]\n",
            "loss: 0.000873  [51200/60000]\n",
            "loss: 0.019544  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.5%, Avg loss: 1.063262 \n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "loss: 0.011541  [    0/60000]\n",
            "loss: 0.002823  [ 6400/60000]\n",
            "loss: 0.042349  [12800/60000]\n",
            "loss: 0.011989  [19200/60000]\n",
            "loss: 0.002126  [25600/60000]\n",
            "loss: 0.008666  [32000/60000]\n",
            "loss: 0.000159  [38400/60000]\n",
            "loss: 0.010227  [44800/60000]\n",
            "loss: 0.042645  [51200/60000]\n",
            "loss: 0.011585  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.5%, Avg loss: 0.967843 \n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "loss: 0.034814  [    0/60000]\n",
            "loss: 0.000790  [ 6400/60000]\n",
            "loss: 0.000537  [12800/60000]\n",
            "loss: 0.000954  [19200/60000]\n",
            "loss: 0.001754  [25600/60000]\n",
            "loss: 0.024510  [32000/60000]\n",
            "loss: 0.000200  [38400/60000]\n",
            "loss: 0.005898  [44800/60000]\n",
            "loss: 0.000142  [51200/60000]\n",
            "loss: 0.002374  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.2%, Avg loss: 0.990345 \n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "loss: 0.004218  [    0/60000]\n",
            "loss: 0.005337  [ 6400/60000]\n",
            "loss: 0.000981  [12800/60000]\n",
            "loss: 0.005414  [19200/60000]\n",
            "loss: 0.000899  [25600/60000]\n",
            "loss: 0.002382  [32000/60000]\n",
            "loss: 0.000295  [38400/60000]\n",
            "loss: 0.013878  [44800/60000]\n",
            "loss: 0.001007  [51200/60000]\n",
            "loss: 0.016738  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.997089 \n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "loss: 0.015521  [    0/60000]\n",
            "loss: 0.026570  [ 6400/60000]\n",
            "loss: 0.002497  [12800/60000]\n",
            "loss: 0.004332  [19200/60000]\n",
            "loss: 0.062070  [25600/60000]\n",
            "loss: 0.015552  [32000/60000]\n",
            "loss: 0.006956  [38400/60000]\n",
            "loss: 0.010142  [44800/60000]\n",
            "loss: 0.000142  [51200/60000]\n",
            "loss: 0.002069  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.6%, Avg loss: 0.998642 \n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "loss: 0.002024  [    0/60000]\n",
            "loss: 0.004588  [ 6400/60000]\n",
            "loss: 0.074641  [12800/60000]\n",
            "loss: 0.018958  [19200/60000]\n",
            "loss: 0.003276  [25600/60000]\n",
            "loss: 0.101376  [32000/60000]\n",
            "loss: 0.008296  [38400/60000]\n",
            "loss: 0.043835  [44800/60000]\n",
            "loss: 0.000129  [51200/60000]\n",
            "loss: 0.005707  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 1.012926 \n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "loss: 0.007112  [    0/60000]\n",
            "loss: 0.001338  [ 6400/60000]\n",
            "loss: 0.000543  [12800/60000]\n",
            "loss: 0.000129  [19200/60000]\n",
            "loss: 0.026121  [25600/60000]\n",
            "loss: 0.066170  [32000/60000]\n",
            "loss: 0.059053  [38400/60000]\n",
            "loss: 0.044382  [44800/60000]\n",
            "loss: 0.012487  [51200/60000]\n",
            "loss: 0.000349  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.5%, Avg loss: 0.973538 \n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "loss: 0.000937  [    0/60000]\n",
            "loss: 0.011058  [ 6400/60000]\n",
            "loss: 0.000939  [12800/60000]\n",
            "loss: 0.000537  [19200/60000]\n",
            "loss: 0.000628  [25600/60000]\n",
            "loss: 0.002497  [32000/60000]\n",
            "loss: 0.001613  [38400/60000]\n",
            "loss: 0.001157  [44800/60000]\n",
            "loss: 0.001296  [51200/60000]\n",
            "loss: 0.015139  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.5%, Avg loss: 1.001270 \n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "loss: 0.001609  [    0/60000]\n",
            "loss: 0.002373  [ 6400/60000]\n",
            "loss: 0.000310  [12800/60000]\n",
            "loss: 0.000287  [19200/60000]\n",
            "loss: 0.032489  [25600/60000]\n",
            "loss: 0.002707  [32000/60000]\n",
            "loss: 0.000267  [38400/60000]\n",
            "loss: 0.000236  [44800/60000]\n",
            "loss: 0.000367  [51200/60000]\n",
            "loss: 0.006404  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.996295 \n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "loss: 0.002859  [    0/60000]\n",
            "loss: 0.026121  [ 6400/60000]\n",
            "loss: 0.000654  [12800/60000]\n",
            "loss: 0.003794  [19200/60000]\n",
            "loss: 0.006816  [25600/60000]\n",
            "loss: 0.009377  [32000/60000]\n",
            "loss: 0.015330  [38400/60000]\n",
            "loss: 0.000250  [44800/60000]\n",
            "loss: 0.000561  [51200/60000]\n",
            "loss: 0.081414  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.1%, Avg loss: 1.030177 \n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "loss: 0.001941  [    0/60000]\n",
            "loss: 0.001739  [ 6400/60000]\n",
            "loss: 0.006387  [12800/60000]\n",
            "loss: 0.002575  [19200/60000]\n",
            "loss: 0.016508  [25600/60000]\n",
            "loss: 0.006068  [32000/60000]\n",
            "loss: 0.011744  [38400/60000]\n",
            "loss: 0.010105  [44800/60000]\n",
            "loss: 0.000449  [51200/60000]\n",
            "loss: 0.018147  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.2%, Avg loss: 1.027627 \n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "loss: 0.042582  [    0/60000]\n",
            "loss: 0.175925  [ 6400/60000]\n",
            "loss: 0.019998  [12800/60000]\n",
            "loss: 0.001414  [19200/60000]\n",
            "loss: 0.000552  [25600/60000]\n",
            "loss: 0.001732  [32000/60000]\n",
            "loss: 0.033604  [38400/60000]\n",
            "loss: 0.017056  [44800/60000]\n",
            "loss: 0.002811  [51200/60000]\n",
            "loss: 0.000824  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.982380 \n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "loss: 0.002704  [    0/60000]\n",
            "loss: 0.000341  [ 6400/60000]\n",
            "loss: 0.008324  [12800/60000]\n",
            "loss: 0.000302  [19200/60000]\n",
            "loss: 0.001799  [25600/60000]\n",
            "loss: 0.013783  [32000/60000]\n",
            "loss: 0.001103  [38400/60000]\n",
            "loss: 0.002118  [44800/60000]\n",
            "loss: 0.002080  [51200/60000]\n",
            "loss: 0.014359  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 0.975945 \n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "loss: 0.008329  [    0/60000]\n",
            "loss: 0.000361  [ 6400/60000]\n",
            "loss: 0.006700  [12800/60000]\n",
            "loss: 0.017398  [19200/60000]\n",
            "loss: 0.000281  [25600/60000]\n",
            "loss: 0.000691  [32000/60000]\n",
            "loss: 0.000265  [38400/60000]\n",
            "loss: 0.000151  [44800/60000]\n",
            "loss: 0.000914  [51200/60000]\n",
            "loss: 0.001002  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.5%, Avg loss: 0.978701 \n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "loss: 0.026492  [    0/60000]\n",
            "loss: 0.000639  [ 6400/60000]\n",
            "loss: 0.000513  [12800/60000]\n",
            "loss: 0.001486  [19200/60000]\n",
            "loss: 0.040772  [25600/60000]\n",
            "loss: 0.015313  [32000/60000]\n",
            "loss: 0.000630  [38400/60000]\n",
            "loss: 0.003009  [44800/60000]\n",
            "loss: 0.000216  [51200/60000]\n",
            "loss: 0.020392  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 0.980072 \n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "loss: 0.000560  [    0/60000]\n",
            "loss: 0.005147  [ 6400/60000]\n",
            "loss: 0.024257  [12800/60000]\n",
            "loss: 0.055350  [19200/60000]\n",
            "loss: 0.002012  [25600/60000]\n",
            "loss: 0.010006  [32000/60000]\n",
            "loss: 0.004173  [38400/60000]\n",
            "loss: 0.005400  [44800/60000]\n",
            "loss: 0.000729  [51200/60000]\n",
            "loss: 0.000717  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 0.993043 \n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "loss: 0.000365  [    0/60000]\n",
            "loss: 0.000142  [ 6400/60000]\n",
            "loss: 0.021478  [12800/60000]\n",
            "loss: 0.003729  [19200/60000]\n",
            "loss: 0.000329  [25600/60000]\n",
            "loss: 0.059091  [32000/60000]\n",
            "loss: 0.012730  [38400/60000]\n",
            "loss: 0.000215  [44800/60000]\n",
            "loss: 0.000225  [51200/60000]\n",
            "loss: 0.003507  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.999309 \n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "loss: 0.002139  [    0/60000]\n",
            "loss: 0.022618  [ 6400/60000]\n",
            "loss: 0.000059  [12800/60000]\n",
            "loss: 0.000475  [19200/60000]\n",
            "loss: 0.001395  [25600/60000]\n",
            "loss: 0.013187  [32000/60000]\n",
            "loss: 0.000228  [38400/60000]\n",
            "loss: 0.000790  [44800/60000]\n",
            "loss: 0.000148  [51200/60000]\n",
            "loss: 0.001346  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 1.018122 \n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "loss: 0.000249  [    0/60000]\n",
            "loss: 0.001303  [ 6400/60000]\n",
            "loss: 0.000429  [12800/60000]\n",
            "loss: 0.000460  [19200/60000]\n",
            "loss: 0.000168  [25600/60000]\n",
            "loss: 0.000071  [32000/60000]\n",
            "loss: 0.000076  [38400/60000]\n",
            "loss: 0.004247  [44800/60000]\n",
            "loss: 0.000368  [51200/60000]\n",
            "loss: 0.005604  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 1.057304 \n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "loss: 0.004155  [    0/60000]\n",
            "loss: 0.000308  [ 6400/60000]\n",
            "loss: 0.004241  [12800/60000]\n",
            "loss: 0.031901  [19200/60000]\n",
            "loss: 0.001182  [25600/60000]\n",
            "loss: 0.000917  [32000/60000]\n",
            "loss: 0.036695  [38400/60000]\n",
            "loss: 0.003123  [44800/60000]\n",
            "loss: 0.000379  [51200/60000]\n",
            "loss: 0.006592  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 1.025190 \n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "loss: 0.008687  [    0/60000]\n",
            "loss: 0.000167  [ 6400/60000]\n",
            "loss: 0.000111  [12800/60000]\n",
            "loss: 0.000936  [19200/60000]\n",
            "loss: 0.000220  [25600/60000]\n",
            "loss: 0.001697  [32000/60000]\n",
            "loss: 0.000259  [38400/60000]\n",
            "loss: 0.000398  [44800/60000]\n",
            "loss: 0.000211  [51200/60000]\n",
            "loss: 0.000499  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.6%, Avg loss: 1.002410 \n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "loss: 0.000213  [    0/60000]\n",
            "loss: 0.002445  [ 6400/60000]\n",
            "loss: 0.005263  [12800/60000]\n",
            "loss: 0.000225  [19200/60000]\n",
            "loss: 0.078390  [25600/60000]\n",
            "loss: 0.031368  [32000/60000]\n",
            "loss: 0.001806  [38400/60000]\n",
            "loss: 0.033749  [44800/60000]\n",
            "loss: 0.000782  [51200/60000]\n",
            "loss: 0.001297  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 1.067019 \n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "loss: 0.042424  [    0/60000]\n",
            "loss: 0.006405  [ 6400/60000]\n",
            "loss: 0.002545  [12800/60000]\n",
            "loss: 0.008857  [19200/60000]\n",
            "loss: 0.004411  [25600/60000]\n",
            "loss: 0.001856  [32000/60000]\n",
            "loss: 0.003235  [38400/60000]\n",
            "loss: 0.000695  [44800/60000]\n",
            "loss: 0.000622  [51200/60000]\n",
            "loss: 0.043612  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 1.053558 \n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "loss: 0.017428  [    0/60000]\n",
            "loss: 0.002043  [ 6400/60000]\n",
            "loss: 0.000047  [12800/60000]\n",
            "loss: 0.028466  [19200/60000]\n",
            "loss: 0.000482  [25600/60000]\n",
            "loss: 0.001257  [32000/60000]\n",
            "loss: 0.006967  [38400/60000]\n",
            "loss: 0.000339  [44800/60000]\n",
            "loss: 0.005326  [51200/60000]\n",
            "loss: 0.000993  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.6%, Avg loss: 1.055499 \n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "loss: 0.084711  [    0/60000]\n",
            "loss: 0.000505  [ 6400/60000]\n",
            "loss: 0.000276  [12800/60000]\n",
            "loss: 0.001013  [19200/60000]\n",
            "loss: 0.007343  [25600/60000]\n",
            "loss: 0.000787  [32000/60000]\n",
            "loss: 0.000302  [38400/60000]\n",
            "loss: 0.000129  [44800/60000]\n",
            "loss: 0.020042  [51200/60000]\n",
            "loss: 0.008788  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 88.9%, Avg loss: 1.100988 \n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "loss: 0.009595  [    0/60000]\n",
            "loss: 0.006258  [ 6400/60000]\n",
            "loss: 0.000764  [12800/60000]\n",
            "loss: 0.007289  [19200/60000]\n",
            "loss: 0.000229  [25600/60000]\n",
            "loss: 0.060236  [32000/60000]\n",
            "loss: 0.000441  [38400/60000]\n",
            "loss: 0.000451  [44800/60000]\n",
            "loss: 0.000248  [51200/60000]\n",
            "loss: 0.000434  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 1.065080 \n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "loss: 0.000400  [    0/60000]\n",
            "loss: 0.028581  [ 6400/60000]\n",
            "loss: 0.010911  [12800/60000]\n",
            "loss: 0.000788  [19200/60000]\n",
            "loss: 0.000476  [25600/60000]\n",
            "loss: 0.002971  [32000/60000]\n",
            "loss: 0.004017  [38400/60000]\n",
            "loss: 0.011569  [44800/60000]\n",
            "loss: 0.000775  [51200/60000]\n",
            "loss: 0.000674  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.8%, Avg loss: 1.049996 \n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "loss: 0.000862  [    0/60000]\n",
            "loss: 0.000287  [ 6400/60000]\n",
            "loss: 0.000186  [12800/60000]\n",
            "loss: 0.015777  [19200/60000]\n",
            "loss: 0.000733  [25600/60000]\n",
            "loss: 0.001657  [32000/60000]\n",
            "loss: 0.021241  [38400/60000]\n",
            "loss: 0.001113  [44800/60000]\n",
            "loss: 0.000221  [51200/60000]\n",
            "loss: 0.001710  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.3%, Avg loss: 1.033785 \n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "loss: 0.000943  [    0/60000]\n",
            "loss: 0.001921  [ 6400/60000]\n",
            "loss: 0.023365  [12800/60000]\n",
            "loss: 0.000382  [19200/60000]\n",
            "loss: 0.029445  [25600/60000]\n",
            "loss: 0.001387  [32000/60000]\n",
            "loss: 0.010366  [38400/60000]\n",
            "loss: 0.001321  [44800/60000]\n",
            "loss: 0.000377  [51200/60000]\n",
            "loss: 0.005043  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.4%, Avg loss: 1.005439 \n",
            "\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "loss: 0.021379  [    0/60000]\n",
            "loss: 0.000532  [ 6400/60000]\n",
            "loss: 0.001546  [12800/60000]\n",
            "loss: 0.001088  [19200/60000]\n",
            "loss: 0.051899  [25600/60000]\n",
            "loss: 0.182958  [32000/60000]\n",
            "loss: 0.001668  [38400/60000]\n",
            "loss: 0.000069  [44800/60000]\n",
            "loss: 0.021259  [51200/60000]\n",
            "loss: 0.071516  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 89.0%, Avg loss: 1.152599 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#Train and test the model\n",
        "epochs = 200\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "    \n",
        "    loss = get_loss(test_dataloader, model, loss_fn)\n",
        "    acc = get_score(test_dataloader, model, loss_fn)\n",
        "    wandb.log({'accuracy': acc, 'Avg loss': loss})\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api = wandb.Api()\n",
        "run = api.run(path_lr_001_sgd)\n",
        "run.summary['epochs'] = 300\n",
        "run.summary[\"batch_size\"] = 64\n",
        "run.summary['optimizer'] = 'SGD'\n",
        "run.summary['learning_rate'] = 0.01\n",
        "run.summary['model'] = 'Base'\n",
        "run.summary.update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api = wandb.Api()\n",
        "run = api.run(path_lr_01_sgd)\n",
        "run.summary['epochs'] = 300\n",
        "run.summary[\"batch_size\"] = 64\n",
        "run.summary['optimizer'] = 'SGD'\n",
        "run.summary['learning_rate'] = 0.1\n",
        "run.summary['model'] = 'Base'\n",
        "run.summary.update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api = wandb.Api()\n",
        "run = api.run(path_lr_1_sgd)\n",
        "run.summary['epochs'] = 109\n",
        "run.summary[\"batch_size\"] = 64\n",
        "run.summary['optimizer'] = 'SGD'\n",
        "run.summary['learning_rate'] = 1\n",
        "run.summary['model'] = 'Base'\n",
        "run.summary.update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "api = wandb.Api()\n",
        "run = api.run(path_lr_1_adam)\n",
        "run.summary['epochs'] = 109\n",
        "run.summary[\"batch_size\"] = 64\n",
        "run.summary['learning_rate'] = 1\n",
        "run.summary['model'] = 'Base'\n",
        "run.summary.update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "api = wandb.Api()\n",
        "run = api.run(path_lr_1_adam)\n",
        "run.config['epochs'] = 109\n",
        "run.config['model'] = 'Base'\n",
        "run.config['learning_rate'] = 1\n",
        "run.config[\"batch_size\"] = 64\n",
        "run.update()    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S16_96RS3O1H"
      },
      "source": [
        "# Question 2: Proposal for Practical Applications (40%)\n",
        "Look for a typical computer vision problem, such as:\n",
        "a. removing noise on the image\n",
        "\n",
        "b. increasing the resolution of the image\n",
        "\n",
        "c. identifying objects in the image\n",
        "\n",
        "d. segmenting the area to which the image belongs\n",
        "\n",
        "e. estimating the depth of an object\n",
        "\n",
        "f. estimating the motion of two object in different frames\n",
        "\n",
        "h. others\n",
        "\n",
        "Discuss possible applications of this problem in life, e.g. image editing systems in your phone, improved quality of the old film, sweeping robot avoiding obstacles, unlocks the face of the mobile phone, identifies the cancer area according to the medical scan image, determines the identity according to the face, identifies the trash can on the road, and the detection system tracks the target object, etc.\n",
        "\n",
        "In this question, you need to do\n",
        "1. Clearly define the problem and describe its application scenarios\n",
        "2. Briefly describe a feasible solution based on image processing and traditional machine learning algorithms.\n",
        "3. Briefly describe a feasible deep learning-based solution.\n",
        "4. Compare the advantages and disadvantages of the two options.\n",
        "\n",
        "Hint1: Submit an individua report for question 2.\n",
        "\n",
        "Hint2: Well orginaze your report.\n",
        "\n",
        "Hint3: You can draw flow chart or inculde other figures for better understanding of your solution.  \n",
        "\n",
        "Please restrict your report within 800 words. In this question, you do not need to implement your solution. You only need to write down a proposal. Please submit this report in a seperate pdf. \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "a3.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
